# app.py

import streamlit as st



# ãƒšãƒ¼ã‚¸è¨­å®šã¯ã€ä»–ã®ã©ã®Streamlitã‚³ãƒãƒ³ãƒ‰ã‚ˆã‚Šã‚‚å…ˆã«ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä¸€ç•ªæœ€åˆã«å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

st.set_page_config(layout="wide", page_title="ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ)")



# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---

import MeCab

import pandas as pd

from collections import Counter

from wordcloud import WordCloud

import matplotlib.pyplot as plt

import matplotlib.font_manager as fm # matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆç®¡ç†

import networkx as nx

from pyvis.network import Network

import re

import os

import numpy as np

from itertools import combinations

# from IPython.core.display import HTML # Streamlitã§ã¯ st.components.v1.html ã‚’ä½¿ç”¨



# --- å®šæ•°å®šç¾© ---

MECABRC_PATH = "/etc/mecabrc"

DICTIONARY_PATH = "/var/lib/mecab/dic/ipadic-utf8"

TAGGER_OPTIONS = f"-r {MECABRC_PATH} -d {DICTIONARY_PATH}"

FONT_PATH_PRIMARY = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf'



# --- MeCab Taggerã®åˆæœŸåŒ– (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨) ---

@st.cache_resource

def initialize_mecab_tagger():

Â  Â  try:

Â  Â  Â  Â  tagger_obj = MeCab.Tagger(TAGGER_OPTIONS)

Â  Â  Â  Â  tagger_obj.parse('')Â 

Â  Â  Â  Â  st.session_state['mecab_tagger_initialized'] = True

Â  Â  Â  Â  print("MeCab Tagger initialized successfully via cache.")

Â  Â  Â  Â  return tagger_obj

Â  Â  except Exception as e_init:

Â  Â  Â  Â  st.error(f"MeCab Taggerã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e_init}")

Â  Â  Â  Â  st.error("ãƒªãƒã‚¸ãƒˆãƒªã« `packages.txt` ãŒæ­£ã—ãè¨­å®šã•ã‚Œã€MeCabé–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

Â  Â  Â  Â  st.session_state['mecab_tagger_initialized'] = False

Â  Â  Â  Â  return None



tagger = initialize_mecab_tagger()



# --- ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã®æ±ºå®šã¨Matplotlibã¸ã®è¨­å®š ---

FONT_PATH_FINAL = None

if 'mecab_tagger_initialized' in st.session_state and st.session_state['mecab_tagger_initialized']:

Â  Â  if os.path.exists(FONT_PATH_PRIMARY):

Â  Â  Â  Â  FONT_PATH_FINAL = FONT_PATH_PRIMARY

Â  Â  Â  Â  st.sidebar.info(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ: {os.path.basename(FONT_PATH_FINAL)}")

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  font_entry = fm.FontEntry(fname=FONT_PATH_FINAL, name=os.path.splitext(os.path.basename(FONT_PATH_FINAL))[0])

Â  Â  Â  Â  Â  Â  if font_entry.name not in [f.name for f in fm.fontManager.ttflist]:

Â  Â  Â  Â  Â  Â  Â  Â  Â fm.fontManager.ttflist.append(font_entry)

Â  Â  Â  Â  Â  Â  plt.rcParams['font.family'] = font_entry.name

Â  Â  Â  Â  Â  Â  print(f"Matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ {font_entry.name} ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")

Â  Â  Â  Â  except Exception as e_font_setting:

Â  Â  Â  Â  Â  Â  st.sidebar.error(f"Matplotlibãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e_font_setting}")

Â  Â  else:

Â  Â  Â  Â  st.sidebar.error(f"æŒ‡å®šIPAãƒ•ã‚©ãƒ³ãƒˆ '{FONT_PATH_PRIMARY}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  font_names_ja = [f.name for f in fm.fontManager.ttflist if any(lang in f.name.lower() for lang in ['ipagp', 'ipag', 'takao', 'noto sans cjk jp', 'hiragino'])]

Â  Â  Â  Â  Â  Â  if font_names_ja:

Â  Â  Â  Â  Â  Â  Â  Â  FONT_PATH_FINAL = fm.findfont(fm.FontProperties(family=font_names_ja[0]))

Â  Â  Â  Â  Â  Â  Â  Â  plt.rcParams['font.family'] = font_names_ja[0]

Â  Â  Â  Â  Â  Â  Â  Â  st.sidebar.info(f"ä»£æ›¿æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ '{font_names_ja[0]}' ({FONT_PATH_FINAL}) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

Â  Â  Â  Â  Â  Â  Â  Â  print(f"Matplotlibã®ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ {font_names_ja[0]} ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â st.sidebar.error("åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒMatplotlibã§è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

Â  Â  Â  Â  except Exception as e_alt_font:

Â  Â  Â  Â  Â  Â  st.sidebar.error(f"ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_alt_font}")

else:

Â  Â  if 'mecab_tagger_initialized' in st.session_state and not st.session_state.get('mecab_tagger_initialized', False) :

Â  Â  Â  Â  st.sidebar.error("MeCabãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")





# --- åˆ†æé–¢æ•°ã®å®šç¾© ---

def perform_morphological_analysis(text_input, tagger_instance):

Â  Â  if tagger_instance is None or not text_input: return []

Â  Â  all_morphemes = []

Â  Â  node = tagger_instance.parseToNode(text_input)

Â  Â  while node:

Â  Â  Â  Â  if node.surface:

Â  Â  Â  Â  Â  Â  features = node.feature.split(',')

Â  Â  Â  Â  Â  Â  all_morphemes.append({

Â  Â  Â  Â  Â  Â  Â  Â  'è¡¨å±¤å½¢': node.surface, 'åŸå½¢': features[6] if features[6] != '*' else node.surface,

Â  Â  Â  Â  Â  Â  Â  Â  'å“è©': features[0], 'å“è©ç´°åˆ†é¡1': features[1], 'å“è©ç´°åˆ†é¡2': features[2],

Â  Â  Â  Â  Â  Â  Â  Â  'å“è©ç´°åˆ†é¡3': features[3], 'æ´»ç”¨å‹': features[4], 'æ´»ç”¨å½¢': features[5],

Â  Â  Â  Â  Â  Â  Â  Â  'èª­ã¿': features[7] if len(features) > 7 and features[7] != '*' else '',

Â  Â  Â  Â  Â  Â  Â  Â  'ç™ºéŸ³': features[8] if len(features) > 8 and features[8] != '*' else ''

Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  node = node.next

Â  Â  return all_morphemes



def generate_word_report(all_morphemes, target_pos_list, stop_words_set):

Â  Â  if not all_morphemes:Â 

Â  Â  Â  Â  return pd.DataFrame(), 0, 0

Â  Â Â 

Â  Â  report_target_morphemes = []

Â  Â  for m in all_morphemes:

Â  Â  Â  Â  if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:

Â  Â  Â  Â  Â  Â  if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['éè‡ªç«‹', 'æ•°', 'ä»£åè©', 'æ¥å°¾', 'ã‚µå¤‰æ¥ç¶š', 'å‰¯è©å¯èƒ½']:

Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  report_target_morphemes.append(m)



Â  Â  if not report_target_morphemes:Â 

Â  Â  Â  Â  return pd.DataFrame(), len(all_morphemes), 0

Â  Â  Â  Â Â 

Â  Â  word_counts = Counter(m['åŸå½¢'] for m in report_target_morphemes)

Â  Â  report_data = []

Â  Â Â 

Â  Â  representative_info_for_report = {}

Â  Â  for m in reversed(report_target_morphemes):Â 

Â  Â  Â  Â  if m['åŸå½¢'] not in representative_info_for_report:

Â  Â  Â  Â  Â  Â  representative_info_for_report[m['åŸå½¢']] = {'å“è©': m['å“è©']}Â 

Â  Â  Â  Â  Â  Â Â 

Â  Â  total_all_morphemes_count_for_freq = len(all_morphemes)

Â  Â  total_report_target_morphemes_count = sum(word_counts.values())



Â  Â  for rank, (word, count) in enumerate(word_counts.most_common(), 1):

Â  Â  Â  Â  info = representative_info_for_report.get(word, {})Â 

Â  Â  Â  Â  frequency = (count / total_all_morphemes_count_for_freq) * 100 if total_all_morphemes_count_for_freq > 0 else 0

Â  Â  Â  Â  report_data.append({

Â  Â  Â  Â  Â  Â  'é †ä½': rank,

Â  Â  Â  Â  Â  Â  'å˜èª (åŸå½¢)': word,

Â  Â  Â  Â  Â  Â  'å‡ºç¾æ•°': count,

Â  Â  Â  Â  Â  Â  'å‡ºç¾é »åº¦ (%)': round(frequency, 3),

Â  Â  Â  Â  Â  Â  'å“è©': info.get('å“è©', '')

Â  Â  Â  Â  Â  Â  # 'å“è©ç´°åˆ†é¡1', 'ä»£è¡¨çš„ãªè¡¨å±¤å½¢', 'ä»£è¡¨çš„ãªèª­ã¿' ã¯å‰Šé™¤æ¸ˆã¿

Â  Â  Â  Â  })

Â  Â  return pd.DataFrame(report_data), total_all_morphemes_count_for_freq, total_report_target_morphemes_count



def generate_wordcloud_image(all_morphemes, font_path_wc, target_pos_list, stop_words_set):

Â  Â  if not all_morphemes: st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã®ãŸã‚ã®å½¢æ…‹ç´ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); return None

Â  Â  if font_path_wc is None or not os.path.exists(font_path_wc): st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã«å¿…è¦ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ '{font_path_wc}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"); return None

Â  Â  wordcloud_words = []

Â  Â  for m in all_morphemes:

Â  Â  Â  Â  if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:

Â  Â  Â  Â  Â  Â  if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['æ•°', 'éè‡ªç«‹', 'ä»£åè©', 'æ¥å°¾']: continue

Â  Â  Â  Â  Â  Â  wordcloud_words.append(m['åŸå½¢'])

Â  Â  wordcloud_text_input_str = " ".join(wordcloud_words)

Â  Â  if not wordcloud_text_input_str.strip(): st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰è¡¨ç¤ºå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚"); return None

Â  Â  try:

Â  Â  Â  Â  wc = WordCloud(font_path=font_path_wc, background_color="white", width=800, height=400, max_words=200, collocations=False, random_state=42).generate(wordcloud_text_input_str)

Â  Â  Â  Â  fig, ax = plt.subplots(figsize=(12,6)); ax.imshow(wc, interpolation='bilinear'); ax.axis("off")

Â  Â  Â  Â  return fig

Â  Â  except Exception as e_wc: st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_wc}"); return None



def generate_cooccurrence_network_html(all_morphemes, text_input_co, tagger_instance, font_path_co, target_pos_list, stop_words_set, node_min_freq, edge_min_freq):

Â  Â  if not all_morphemes or tagger_instance is None or not text_input_co.strip(): st.info("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚"); return None

Â  Â  if font_path_co is None or not os.path.exists(font_path_co): st.error(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ©ãƒ™ãƒ«è¡¨ç¤ºã«å¿…è¦ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ '{font_path_co}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"); return None

Â  Â  temp_words_for_nodes = []

Â  Â  for m in all_morphemes:

Â  Â  Â  Â  if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:

Â  Â  Â  Â  Â  Â  if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['éè‡ªç«‹', 'æ•°', 'ä»£åè©', 'æ¥å°¾', 'ã‚µå¤‰æ¥ç¶š', 'å‰¯è©å¯èƒ½']: continue

Â  Â  Â  Â  Â  Â  if len(m['åŸå½¢']) < 2 and m['å“è©'] != 'åè©': continue

Â  Â  Â  Â  Â  Â  temp_words_for_nodes.append(m['åŸå½¢'])

Â  Â  word_counts = Counter(temp_words_for_nodes); node_candidates = {word: count for word, count in word_counts.items() if count >= node_min_freq}

Â  Â  if len(node_candidates) < 2: st.info(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒãƒ¼ãƒ‰ã¨ãªã‚‹å˜èªï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰ãŒ2ã¤æœªæº€ã§ã™ã€‚"); return None

Â  Â  sentences = re.split(r'[ã€‚\nï¼ï¼Ÿ]+', text_input_co); sentences = [s.strip() for s in sentences if s.strip()]

Â  Â  cooccurrence_counts_map = Counter()

Â  Â  for sentence in sentences:

Â  Â  Â  Â  node_s = tagger_instance.parseToNode(sentence); words_in_sentence = []

Â  Â  Â  Â  while node_s:

Â  Â  Â  Â  Â  Â  if node_s.surface:

Â  Â  Â  Â  Â  Â  Â  Â  features = node_s.feature.split(','); original_form = features[6] if features[6] != '*' else node_s.surface

Â  Â  Â  Â  Â  Â  Â  Â  if original_form in node_candidates: words_in_sentence.append(original_form)

Â  Â  Â  Â  Â  Â  node_s = node_s.next

Â  Â  Â  Â  for pair in combinations(sorted(list(set(words_in_sentence))), 2): cooccurrence_counts_map[pair] += 1

Â  Â  if not cooccurrence_counts_map: st.info("å…±èµ·ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return None

Â  Â  font_name_pyvis_graph = os.path.splitext(os.path.basename(font_path_co))[0]

Â  Â  if font_name_pyvis_graph.lower() == 'ipagp': font_name_pyvis_graph = 'IPAPGothic'

Â  Â  elif font_name_pyvis_graph.lower() == 'ipamp': font_name_pyvis_graph = 'IPAPMincho'

Â  Â  net_graph = Network(notebook=True, height="750px", width="100%", directed=False, bgcolor="#F5F5F5", font_color="#333333")

Â  Â  for word, count in node_candidates.items():

Â  Â  Â  Â  node_s_size = int(np.sqrt(count) * 10 + 10)

Â  Â  Â  Â  net_graph.add_node(word, label=word, size=node_s_size, title=f"{word} (å‡ºç¾æ•°: {count})", font={'face': font_name_pyvis_graph, 'size': 14, 'color': '#333333'}, borderWidth=1, color={'border': '#666666', 'background': '#D2E5FF'})

Â  Â  added_edge_num = 0

Â  Â  for pair_nodes, freq_cooc in cooccurrence_counts_map.items():

Â  Â  Â  Â  if freq_cooc >= edge_min_freq:

Â  Â  Â  Â  Â  Â  edge_w = float(np.log1p(freq_cooc) * 1.5 + 0.5)

Â  Â  Â  Â  Â  Â  net_graph.add_edge(pair_nodes[0], pair_nodes[1], value=edge_w, title=f"å…±èµ·: {freq_cooc}å›", color={'color': '#cccccc', 'highlight': '#848484', 'opacity':0.6}); added_edge_num +=1

Â  Â  if added_edge_num == 0: st.info(f"è¡¨ç¤ºå¯¾è±¡ã®å…±èµ·ãƒšã‚¢ï¼ˆå…±èµ·å›æ•° {edge_min_freq} å›ä»¥ä¸Šï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return None

Â  Â  options_js_str = """ var options = {"interaction": {"navigationButtons": false, "keyboard": {"enabled": false}}, "manipulation": {"enabled": false}, "configure": {"enabled": false}, "physics": {"enabled": true, "barnesHut": {"gravitationalConstant": -30000, "centralGravity": 0.1, "springLength": 150, "springConstant": 0.03, "damping": 0.09, "avoidOverlap": 0.5}, "solver": "barnesHut", "stabilization": {"iterations": 500}}}; """

Â  Â  try: net_graph.set_options(options_js_str)

Â  Â  except Exception as e_set_opt: st.warning(f"Pyvisã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šã§è»½å¾®ãªã‚¨ãƒ©ãƒ¼: {e_set_opt}")

Â  Â  net_graph.show_buttons(filter_=False)

Â  Â  return net_graph.generate_html(name="temp_cooc_net_streamlit.html", notebook=True)



def perform_kwic_search(all_morphemes, keyword_str, search_key_type_str, window_int):

Â  Â  if not keyword_str.strip() or not all_morphemes: return []

Â  Â  kwic_results_data = []

Â  Â  for i, morpheme_item in enumerate(all_morphemes):

Â  Â  Â  Â  # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å½¢æ…‹ç´ ã®åŸå½¢/è¡¨å±¤å½¢ã‚’æ¯”è¼ƒã™ã‚‹å‰ã«ã€ä¸¡æ–¹ã‚’å°æ–‡å­—åŒ–ã—ã¦å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ã‚ˆã†ã«ã™ã‚‹

Â  Â  Â  Â  target_text_in_morpheme = morpheme_item[search_key_type_str].lower()

Â  Â  Â  Â  keyword_to_compare = keyword_str.lower()

Â  Â  Â  Â Â 

Â  Â  Â  Â  if target_text_in_morpheme == keyword_to_compare:

Â  Â  Â  Â  Â  Â  left_start_idx = max(0, i - window_int); left_ctx_str = "".join(m['è¡¨å±¤å½¢'] for m in all_morphemes[left_start_idx:i])

Â  Â  Â  Â  Â  Â  kw_surface = morpheme_item['è¡¨å±¤å½¢']; right_end_idx = min(len(all_morphemes), i + 1 + window_int)

Â  Â  Â  Â  Â  Â  right_ctx_str = "".join(m['è¡¨å±¤å½¢'] for m in all_morphemes[i+1:right_end_idx])

Â  Â  Â  Â  Â  Â  kwic_results_data.append({'å·¦æ–‡è„ˆ': left_ctx_str, 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰': kw_surface, 'å³æ–‡è„ˆ': right_ctx_str})

Â  Â  return kwic_results_data



# --- Streamlit UIã®ãƒ¡ã‚¤ãƒ³éƒ¨åˆ† ---

st.title("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ)")

st.markdown("æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ã€å½¢æ…‹ç´ è§£æã€å˜èªãƒ¬ãƒãƒ¼ãƒˆã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€KWICæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")



# â˜…â˜…â˜… ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’ç©ºã®ã‚»ãƒƒãƒˆã« â˜…â˜…â˜…

DEFAULT_STOP_WORDS_SET = set()



st.sidebar.header("âš™ï¸ åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³")

st.sidebar.markdown("**å“è©é¸æŠ (å„åˆ†æå…±é€š)**")

default_target_pos = ['åè©', 'å‹•è©', 'å½¢å®¹è©']

report_target_pos_selected = st.sidebar.multiselect("å˜èªãƒ¬ãƒãƒ¼ãƒˆ: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©', 'æ„Ÿå‹•è©', 'é€£ä½“è©'], default=default_target_pos)

wc_target_pos_selected = st.sidebar.multiselect("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©', 'æ„Ÿå‹•è©'], default=default_target_pos)

net_target_pos_selected = st.sidebar.multiselect("å…±èµ·Net: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©'], default=default_target_pos)



st.sidebar.markdown("**ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š**")

# â˜…â˜…â˜… ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ç©ºã« â˜…â˜…â˜…

custom_stopwords_input_str = st.sidebar.text_area("å…±é€šã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ (åŸå½¢ã‚’ã‚«ãƒ³ãƒã‚„æ”¹è¡ŒåŒºåˆ‡ã‚Šã§å…¥åŠ›):",Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â value="", # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ç©ºæ–‡å­—åˆ—ã«å¤‰æ›´

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â help="ã“ã“ã«å…¥åŠ›ã—ãŸå˜èªï¼ˆåŸå½¢ï¼‰ãŒã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦å‡¦ç†ã•ã‚Œã¾ã™ã€‚")

final_stop_words_set = DEFAULT_STOP_WORDS_SET.copy() # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãŒç©ºãªã®ã§ã€å®Ÿè³ªãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ã¿

if custom_stopwords_input_str.strip():

Â  Â  custom_list_sw = [word.strip().lower() for word in re.split(r'[,\n]', custom_stopwords_input_str) if word.strip()]

Â  Â  final_stop_words_set.update(custom_list_sw)

st.sidebar.caption(f"é©ç”¨ã•ã‚Œã‚‹ç·ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰æ•°: {len(final_stop_words_set)}")



st.sidebar.markdown("---")

st.sidebar.markdown("**å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è©³ç´°è¨­å®š**")

network_node_min_freq_val = st.sidebar.slider("ãƒãƒ¼ãƒ‰æœ€ä½å‡ºç¾æ•°:", 1, 20, 2, key="net_node_freq_slider_main")

network_edge_min_freq_val = st.sidebar.slider("ã‚¨ãƒƒã‚¸æœ€ä½å…±èµ·æ•°:", 1, 10, 2, key="net_edge_freq_slider_main")



main_text_input_area = st.text_area("ğŸ“ åˆ†æã—ãŸã„æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", height=250,Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â value="ã“ã‚Œã¯Streamlitã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«ã§ã™ã€‚æ—¥æœ¬èªã®å½¢æ…‹ç´ è§£æã‚’è¡Œã„ã€å˜èªã®å‡ºç¾é »åº¦ãƒ¬ãƒãƒ¼ãƒˆã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãã—ã¦KWICï¼ˆæ–‡è„ˆä»˜ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰ãªã©ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚æ§˜ã€…ãªæ–‡ç« ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„ã€‚")



analyze_button_clicked = st.button("åˆ†æå®Ÿè¡Œ", type="primary", use_container_width=True)



if analyze_button_clicked:

Â  Â  if not main_text_input_area.strip():

Â  Â  Â  Â  st.warning("åˆ†æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

Â  Â  elif tagger is None or not st.session_state.get('mecab_tagger_initialized', False):

Â  Â  Â  Â  st.error("MeCab TaggerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã™ã‚‹ã‹ã€Streamlit Cloudã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

Â  Â  else:

Â  Â  Â  Â  with st.spinner("å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œä¸­... ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"):

Â  Â  Â  Â  Â  Â  morphemes_data_list = perform_morphological_analysis(main_text_input_area, tagger)

Â  Â  Â  Â Â 

Â  Â  Â  Â  if not morphemes_data_list:

Â  Â  Â  Â  Â  Â  st.error("å½¢æ…‹ç´ è§£æã«å¤±æ•—ã—ãŸã‹ã€çµæœãŒç©ºã§ã™ã€‚å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  st.success(f"å½¢æ…‹ç´ è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·å½¢æ…‹ç´ æ•°: {len(morphemes_data_list)}")

Â  Â  Â  Â  Â  Â  st.markdown("---")



Â  Â  Â  Â  Â  Â  # â˜…â˜…â˜… æ„Ÿæƒ…åˆ†æã‚¿ãƒ–ã‚’å‰Šé™¤ â˜…â˜…â˜…

Â  Â  Â  Â  Â  Â  tab_report_view, tab_wc_view, tab_network_view, tab_kwic_view = st.tabs([

Â  Â  Â  Â  Â  Â  Â  Â  "ğŸ“Š å˜èªå‡ºç¾ãƒ¬ãƒãƒ¼ãƒˆ", "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ” KWICæ¤œç´¢"

Â  Â  Â  Â  Â  Â  ])



Â  Â  Â  Â  Â  Â  with tab_report_view:

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("å˜èªå‡ºç¾ãƒ¬ãƒãƒ¼ãƒˆ")

Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­..."):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_report_to_show, total_morphs, total_target_morphs = generate_word_report(morphemes_data_list, report_target_pos_selected, final_stop_words_set)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"ç·å½¢æ…‹ç´ æ•°: {total_morphs} | ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®ç•°ãªã‚Šèªæ•°: {len(df_report_to_show)} | ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®å»¶ã¹èªæ•°: {total_target_morphs}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not df_report_to_show.empty:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # â˜…â˜…â˜… å‡ºç¾æ•°ã®åˆ—ã«ãƒŸãƒ‹ã‚°ãƒ©ãƒ•ã‚’é©ç”¨ (ä»¥å‰ã®ã‚³ãƒ¼ãƒ‰ã§æ­£ã—ãå®Ÿè£…æ¸ˆã¿ã®ã¯ãš) â˜…â˜…â˜…

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(df_report_to_show.style.bar(subset=['å‡ºç¾æ•°'], align='left', color='#90EE90')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â .format({'å‡ºç¾é »åº¦ (%)': "{:.3f}%"}))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  with tab_wc_view:

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")

Â  Â  Â  Â  Â  Â  Â  Â  if FONT_PATH_FINAL:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆä¸­..."):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig_wc_to_show = generate_wordcloud_image(morphemes_data_list, FONT_PATH_FINAL, wc_target_pos_selected, final_stop_words_set)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if fig_wc_to_show: st.pyplot(fig_wc_to_show)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆ: {os.path.basename(FONT_PATH_FINAL) if FONT_PATH_FINAL else 'æœªè¨­å®š'}")

Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  with tab_network_view:

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")

Â  Â  Â  Â  Â  Â  Â  Â  if FONT_PATH_FINAL:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆä¸­..."):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  html_cooc_to_show = generate_cooccurrence_network_html(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  morphemes_data_list, main_text_input_area, tagger, FONT_PATH_FINAL,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  net_target_pos_selected, final_stop_words_set,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  network_node_min_freq_val, network_edge_min_freq_val)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if html_cooc_to_show: st.components.v1.html(html_cooc_to_show, height=750, scrolling=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆ (ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«): {os.path.basename(FONT_PATH_FINAL) if FONT_PATH_FINAL else 'æœªè¨­å®š'}")

Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  with tab_kwic_view:

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("KWICæ¤œç´¢ (æ–‡è„ˆä»˜ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢)")

Â  Â  Â  Â  Â  Â  Â  Â  if 'kwic_keyword' not in st.session_state: st.session_state.kwic_keyword = ""

Â  Â  Â  Â  Â  Â  Â  Â  if 'kwic_mode_idx' not in st.session_state: st.session_state.kwic_mode_idx = 0

Â  Â  Â  Â  Â  Â  Â  Â  if 'kwic_window_val' not in st.session_state: st.session_state.kwic_window_val = 5



Â  Â  Â  Â  Â  Â  Â  Â  kwic_keyword_input_val = st.text_input("KWICæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:", value=st.session_state.kwic_keyword, placeholder="æ¤œç´¢ã—ãŸã„å˜èª(åŸå½¢æ¨å¥¨)...", key="kwic_keyword_input_field_tab")

Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.kwic_keyword = kwic_keyword_input_val



Â  Â  Â  Â  Â  Â  Â  Â  kwic_search_mode_options_list = ("åŸå½¢ä¸€è‡´", "è¡¨å±¤å½¢ä¸€è‡´"); kwic_search_mode_selected_val = st.radio("KWICæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰:", kwic_search_mode_options_list, index=st.session_state.kwic_mode_idx, key="kwic_mode_radio_field_tab")

Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.kwic_mode_idx = kwic_search_mode_options_list.index(kwic_search_mode_selected_val)



Â  Â  Â  Â  Â  Â  Â  Â  kwic_window_val_set = st.slider("KWICè¡¨ç¤ºæ–‡è„ˆã®å½¢æ…‹ç´ æ•° (å‰å¾Œå„):", 1, 15, st.session_state.kwic_window_val, key="kwic_window_slider_field_tab")

Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.kwic_window_val = kwic_window_val_set



Â  Â  Â  Â  Â  Â  Â  Â  if kwic_keyword_input_val.strip():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  search_key_type_for_kwic_val = 'åŸå½¢' if kwic_search_mode_selected_val == "åŸå½¢ä¸€è‡´" else 'è¡¨å±¤å½¢'

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  kw_to_search = kwic_keyword_input_val.strip()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(f"ã€Œ{kw_to_search}ã€ã‚’æ¤œç´¢ä¸­..."):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results_kwic_list_data = perform_kwic_search(morphemes_data_list, kw_to_search, search_key_type_for_kwic_val, kwic_window_val_set)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if results_kwic_list_data:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"ã€Œ{kw_to_search}ã€ã®æ¤œç´¢çµæœ ({len(results_kwic_list_data)}ä»¶):"); df_kwic_to_display_final = pd.DataFrame(results_kwic_list_data); st.dataframe(df_kwic_to_display_final)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.info(f"ã€Œ{kw_to_search}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆç¾åœ¨ã®æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã«ãŠã„ã¦ï¼‰ã€‚")



# --- ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ± ---

st.sidebar.markdown("---")

st.sidebar.info("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ) v0.3") # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å°‘ã—ä¸Šã’ã¾ã—ãŸ
