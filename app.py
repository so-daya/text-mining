# app.py
import streamlit as st

# ãƒšãƒ¼ã‚¸è¨­å®šã¯ã€ä»–ã®ã©ã®Streamlitã‚³ãƒãƒ³ãƒ‰ã‚ˆã‚Šã‚‚å…ˆã«ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä¸€ç•ªæœ€åˆã«å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
st.set_page_config(layout="wide", page_title="ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ)")

# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import MeCab
import pandas as pd
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm # matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆç®¡ç†
import networkx as nx
from pyvis.network import Network
import re
import os
import numpy as np
from itertools import combinations
# from IPython.core.display import HTML # Streamlitã§ã¯ st.components.v1.html ã‚’ä½¿ç”¨

# --- å®šæ•°å®šç¾© ---
MECABRC_PATH = "/etc/mecabrc"
DICTIONARY_PATH = "/var/lib/mecab/dic/ipadic-utf8"
TAGGER_OPTIONS = f"-r {MECABRC_PATH} -d {DICTIONARY_PATH}"
# packages.txtã§fonts-ipafont-gothicã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå ´åˆã®IPA Pã‚´ã‚·ãƒƒã‚¯ã®ãƒ‘ã‚¹
FONT_PATH_PRIMARY = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf' 

# --- MeCab Taggerã®åˆæœŸåŒ– (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨) ---
@st.cache_resource # Streamlitã®æ–°ã—ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def initialize_mecab_tagger():
    try:
        tagger_obj = MeCab.Tagger(TAGGER_OPTIONS)
        tagger_obj.parse('') # åˆæœŸåŒ–ã®ãŠã¾ã˜ãªã„
        st.session_state['mecab_tagger_initialized'] = True # åˆæœŸåŒ–æˆåŠŸãƒ•ãƒ©ã‚°
        print("MeCab Tagger initialized successfully via cache.")
        return tagger_obj
    except Exception as e_init:
        st.error(f"MeCab Taggerã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e_init}")
        st.error("ãƒªãƒã‚¸ãƒˆãƒªã« `packages.txt` ãŒæ­£ã—ãè¨­å®šã•ã‚Œã€MeCabé–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.session_state['mecab_tagger_initialized'] = False
        return None

tagger = initialize_mecab_tagger()

# --- ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã®æ±ºå®šã¨Matplotlibã¸ã®è¨­å®š ---
FONT_PATH_FINAL = None
if 'mecab_tagger_initialized' in st.session_state and st.session_state['mecab_tagger_initialized']:
    if os.path.exists(FONT_PATH_PRIMARY):
        FONT_PATH_FINAL = FONT_PATH_PRIMARY
        st.sidebar.info(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ: {os.path.basename(FONT_PATH_FINAL)}") # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«æƒ…å ±è¡¨ç¤º
        try:
            # Matplotlibã«ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã‚’ç™»éŒ²ã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦è¨­å®š
            font_entry = fm.FontEntry(fname=FONT_PATH_FINAL, name=os.path.splitext(os.path.basename(FONT_PATH_FINAL))[0])
            fm.fontManager.ttflist.append(font_entry)
            plt.rcParams['font.family'] = font_entry.name
            print(f"Matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ {font_entry.name} ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")
        except Exception as e_font_setting:
            st.sidebar.error(f"Matplotlibãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e_font_setting}")
            # FONT_PATH_FINAL ã¯ãã®ã¾ã¾ä½¿ã†ãŒã€matplotlibã§ã®æ—¥æœ¬èªè¡¨ç¤ºã¯æœŸå¾…ã§ããªã„
    else:
        st.sidebar.error(f"æŒ‡å®šIPAãƒ•ã‚©ãƒ³ãƒˆ '{FONT_PATH_PRIMARY}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        # ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ã‚·ã‚¹ãƒ†ãƒ ãŒèªè­˜ã§ãã‚‹ã‚‚ã®ã‚’æ¢ã™ (ã‚ˆã‚Šæ±ç”¨çš„ãªæ–¹æ³•)
        try:
            font_names_ja = [f.name for f in fm.fontManager.ttflist if any(lang in f.name.lower() for lang in ['ipagp', 'ipag', 'takao', 'noto sans cjk jp', 'hiragino'])]
            if font_names_ja:
                FONT_PATH_FINAL = fm.findfont(fm.FontProperties(family=font_names_ja[0])) # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ
                plt.rcParams['font.family'] = font_names_ja[0]
                st.sidebar.info(f"ä»£æ›¿æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ '{font_names_ja[0]}' ({FONT_PATH_FINAL}) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                print(f"Matplotlibã®ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ {font_names_ja[0]} ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")
            else:
                 st.sidebar.error("åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒMatplotlibã§è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e_alt_font:
            st.sidebar.error(f"ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_alt_font}")
else:
    if 'mecab_tagger_initialized' in st.session_state and not st.session_state.get('mecab_tagger_initialized', False) :
        st.sidebar.error("MeCabãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")


# --- åˆ†æé–¢æ•°ã®å®šç¾© ---
def perform_morphological_analysis(text_input, tagger_instance):
    if tagger_instance is None or not text_input: return []
    all_morphemes = []
    node = tagger_instance.parseToNode(text_input)
    while node:
        if node.surface:
            features = node.feature.split(',')
            all_morphemes.append({
                'è¡¨å±¤å½¢': node.surface, 'åŸå½¢': features[6] if features[6] != '*' else node.surface,
                'å“è©': features[0], 'å“è©ç´°åˆ†é¡1': features[1], 'å“è©ç´°åˆ†é¡2': features[2],
                'å“è©ç´°åˆ†é¡3': features[3], 'æ´»ç”¨å‹': features[4], 'æ´»ç”¨å½¢': features[5],
                'èª­ã¿': features[7] if len(features) > 7 and features[7] != '*' else '',
                'ç™ºéŸ³': features[8] if len(features) > 8 and features[8] != '*' else ''
            })
        node = node.next
    return all_morphemes

def generate_word_report(all_morphemes, target_pos_list, stop_words_set):
    if not all_morphemes: return pd.DataFrame(), 0, 0
    report_target_morphemes = []
    for m in all_morphemes:
        if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:
            if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['éè‡ªç«‹', 'æ•°', 'ä»£åè©', 'æ¥å°¾', 'ã‚µå¤‰æ¥ç¶š', 'å‰¯è©å¯èƒ½']:
                continue
            report_target_morphemes.append(m)
    if not report_target_morphemes: return pd.DataFrame(), len(all_morphemes), 0
    word_counts = Counter(m['åŸå½¢'] for m in report_target_morphemes)
    report_data = []
    representative_info_for_report = {m['åŸå½¢']: m for m in reversed(report_target_morphemes)}
    total_all_morphemes_count_for_freq = len(all_morphemes)
    total_report_target_morphemes_count = sum(word_counts.values())

    for rank, (word, count) in enumerate(word_counts.most_common(), 1):
        info = representative_info_for_report.get(word, {})
        frequency = (count / total_all_morphemes_count_for_freq) * 100 if total_all_morphemes_count_for_freq > 0 else 0
        report_data.append({
            'é †ä½': rank, 'å˜èª (åŸå½¢)': word, 'å‡ºç¾æ•°': count,
            'å‡ºç¾é »åº¦ (%)': round(frequency, 3), 'å“è©': info.get('å“è©', ''),
            'å“è©ç´°åˆ†é¡1': info.get('å“è©ç´°åˆ†é¡1', ''), 'ä»£è¡¨çš„ãªè¡¨å±¤å½¢': info.get('è¡¨å±¤å½¢', ''),
            'ä»£è¡¨çš„ãªèª­ã¿': info.get('èª­ã¿', '')
        })
    return pd.DataFrame(report_data), total_all_morphemes_count_for_freq, total_report_target_morphemes_count

def generate_wordcloud_image(all_morphemes, font_path_wc, target_pos_list, stop_words_set):
    if not all_morphemes:
        st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã®ãŸã‚ã®å½¢æ…‹ç´ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return None
    if font_path_wc is None or not os.path.exists(font_path_wc):
        st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã«å¿…è¦ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ '{font_path_wc}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    wordcloud_words = []
    for m in all_morphemes:
        if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:
            if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['æ•°', 'éè‡ªç«‹', 'ä»£åè©', 'æ¥å°¾']:
                continue
            wordcloud_words.append(m['åŸå½¢'])
    wordcloud_text_input_str = " ".join(wordcloud_words)
    if not wordcloud_text_input_str.strip():
        st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰è¡¨ç¤ºå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚")
        return None
    try:
        wc = WordCloud(
            font_path=font_path_wc, background_color="white", width=800, height=400,
            max_words=200, collocations=False, random_state=42
        ).generate(wordcloud_text_input_str)
        fig, ax = plt.subplots(figsize=(12,6))
        ax.imshow(wc, interpolation='bilinear'); ax.axis("off")
        return fig
    except Exception as e_wc:
        st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_wc}")
        return None

def generate_cooccurrence_network_html(all_morphemes, text_input_co, tagger_instance, font_path_co, target_pos_list, stop_words_set, node_min_freq, edge_min_freq):
    if not all_morphemes or tagger_instance is None or not text_input_co.strip():
        st.info("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        return None
    if font_path_co is None or not os.path.exists(font_path_co):
        st.error(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ©ãƒ™ãƒ«è¡¨ç¤ºã«å¿…è¦ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ '{font_path_co}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    temp_words_for_nodes = []
    for m in all_morphemes:
        if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:
            if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['éè‡ªç«‹', 'æ•°', 'ä»£åè©', 'æ¥å°¾', 'ã‚µå¤‰æ¥ç¶š', 'å‰¯è©å¯èƒ½']: continue
            if len(m['åŸå½¢']) < 2 and m['å“è©'] != 'åè©': continue
            temp_words_for_nodes.append(m['åŸå½¢'])
    word_counts = Counter(temp_words_for_nodes)
    node_candidates = {word: count for word, count in word_counts.items() if count >= node_min_freq}
    if len(node_candidates) < 2:
        st.info(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒãƒ¼ãƒ‰ã¨ãªã‚‹å˜èªï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰ãŒ2ã¤æœªæº€ã§ã™ã€‚")
        return None
    sentences = re.split(r'[ã€‚\nï¼ï¼Ÿ]+', text_input_co); sentences = [s.strip() for s in sentences if s.strip()]
    cooccurrence_counts_map = Counter()
    for sentence in sentences:
        node_s = tagger_instance.parseToNode(sentence); words_in_sentence = []
        while node_s:
            if node_s.surface:
                features = node_s.feature.split(','); original_form = features[6] if features[6] != '*' else node_s.surface
                if original_form in node_candidates: words_in_sentence.append(original_form)
            node_s = node_s.next
        for pair in combinations(sorted(list(set(words_in_sentence))), 2): cooccurrence_counts_map[pair] += 1
    if not cooccurrence_counts_map:
        st.info("å…±èµ·ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    
    font_name_pyvis_graph = os.path.splitext(os.path.basename(font_path_co))[0]
    if font_name_pyvis_graph.lower() == 'ipagp': font_name_pyvis_graph = 'IPAPGothic'
    elif font_name_pyvis_graph.lower() == 'ipamp': font_name_pyvis_graph = 'IPAPMincho'
    
    net_graph = Network(notebook=True, height="750px", width="100%", directed=False, bgcolor="#F5F5F5", font_color="#333333")
    for word, count in node_candidates.items():
        node_s_size = int(np.sqrt(count) * 10 + 10)
        net_graph.add_node(word, label=word, size=node_s_size, title=f"{word} (å‡ºç¾æ•°: {count})",
                           font={'face': font_name_pyvis_graph, 'size': 14, 'color': '#333333'},
                           borderWidth=1, color={'border': '#666666', 'background': '#D2E5FF'})
    added_edge_num = 0
    for pair_nodes, freq_cooc in cooccurrence_counts_map.items():
        if freq_cooc >= edge_min_freq:
            edge_w = float(np.log1p(freq_cooc) * 1.5 + 0.5)
            net_graph.add_edge(pair_nodes[0], pair_nodes[1], value=edge_w, title=f"å…±èµ·: {freq_cooc}å›",
                               color={'color': '#cccccc', 'highlight': '#848484', 'opacity':0.6})
            added_edge_num +=1
    if added_edge_num == 0:
        st.info(f"è¡¨ç¤ºå¯¾è±¡ã®å…±èµ·ãƒšã‚¢ï¼ˆå…±èµ·å›æ•° {edge_min_freq} å›ä»¥ä¸Šï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    options_js_str = """ var options = {"interaction": {"navigationButtons": false, "keyboard": {"enabled": false}}, "manipulation": {"enabled": false}, "configure": {"enabled": false}, "physics": {"enabled": true, "barnesHut": {"gravitationalConstant": -30000, "centralGravity": 0.1, "springLength": 150, "springConstant": 0.03, "damping": 0.09, "avoidOverlap": 0.5}, "solver": "barnesHut", "stabilization": {"iterations": 500}}}; """
    try:
        net_graph.set_options(options_js_str)
    except Exception as e_set_opt:
        st.warning(f"Pyvisã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šã§è»½å¾®ãªã‚¨ãƒ©ãƒ¼: {e_set_opt}")
    net_graph.show_buttons(filter_=False)
    return net_graph.generate_html(name="temp_cooc_net_streamlit.html", notebook=True)

def perform_kwic_search(all_morphemes, keyword_str, search_key_type_str, window_int):
    if not keyword_str.strip() or not all_morphemes: return []
    kwic_results_data = []
    for i, morpheme_item in enumerate(all_morphemes):
        if morpheme_item[search_key_type_str].lower() == keyword_str.lower(): # æ¤œç´¢æ™‚ã«å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ã‚ˆã†ã«å¤‰æ›´
            left_start_idx = max(0, i - window_int)
            left_ctx_str = "".join(m['è¡¨å±¤å½¢'] for m in all_morphemes[left_start_idx:i])
            kw_surface = morpheme_item['è¡¨å±¤å½¢']
            right_end_idx = min(len(all_morphemes), i + 1 + window_int)
            right_ctx_str = "".join(m['è¡¨å±¤å½¢'] for m in all_morphemes[i+1:right_end_idx])
            kwic_results_data.append({'å·¦æ–‡è„ˆ': left_ctx_str, 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰': kw_surface, 'å³æ–‡è„ˆ': right_ctx_str})
    return kwic_results_data

# --- Streamlit UIã®ãƒ¡ã‚¤ãƒ³éƒ¨åˆ† ---
st.title("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ)")
st.markdown("æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ã€å½¢æ…‹ç´ è§£æã€å˜èªãƒ¬ãƒãƒ¼ãƒˆã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€KWICæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")

# --- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨UI ---
DEFAULT_STOP_WORDS_SET = {
    "ã™ã‚‹", "ã‚ã‚‹", "ã„ã‚‹", "ãªã‚‹", "ã„ã†", "ã§ãã‚‹", "æ€ã†", "ã‚„ã‚‹", "ãªã„", "ã‚ˆã„", "è‰¯ã„",
    "å¤§ãã„", "å°ã•ã„", "é«˜ã„", "ä½ã„", "å¬‰ã—ã„", "æ¥½ã—ã„", "æ‚²ã—ã„", "åŒã˜", "æ§˜ã€…",
    "ã“ã¨", "ã‚‚ã®", "ãŸã‚", "ã‚ˆã†", "çš„", "çš„ã ", "ã¨ã", "ã¨ã“ã‚", "ã»ã†", "ãªã‹", "ã†ã¡",
    "ç§", "ã‚ãªãŸ", "å½¼", "å½¼å¥³", "ã“ã‚Œ", "ãã‚Œ", "ã‚ã‚Œ", "ã“ã“", "ãã“", "ã‚ãã“", "æ–¹", "ç‚º",
    "éå¸¸", "å¤§å¤‰", "å°‘ã—", "ã‹ãªã‚Š", "è‰²ã€…", "ã„ã¤ã‚‚", "ã„ãŸã ã", "/", ":", "ã‚Œã‚‹", "\""
}

st.sidebar.header("âš™ï¸ åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³")
st.sidebar.markdown("**å“è©é¸æŠ (å„åˆ†æå…±é€š)**")
# å…±é€šã®å¯¾è±¡å“è©ãƒªã‚¹ãƒˆã‚’ä½¿ã†ã‹ã€å€‹åˆ¥ã«è¨­å®šã™ã‚‹ã‹é¸æŠã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
# ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ã€ãƒ¬ãƒãƒ¼ãƒˆã€WCã€Netã§ä¼¼ãŸã‚ˆã†ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’è¨­å®š
default_target_pos = ['åè©', 'å‹•è©', 'å½¢å®¹è©']
report_target_pos_selected = st.sidebar.multiselect("å˜èªãƒ¬ãƒãƒ¼ãƒˆ: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©', 'æ„Ÿå‹•è©', 'é€£ä½“è©'], default=default_target_pos)
wc_target_pos_selected = st.sidebar.multiselect("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©', 'æ„Ÿå‹•è©'], default=default_target_pos)
net_target_pos_selected = st.sidebar.multiselect("å…±èµ·Net: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©'], default=default_target_pos)

st.sidebar.markdown("**ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š**")
custom_stopwords_input_str = st.sidebar.text_area("å…±é€šã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ (åŸå½¢ã‚’ã‚«ãƒ³ãƒã‚„æ”¹è¡ŒåŒºåˆ‡ã‚Šã§è¿½åŠ ):", 
                                             value="ã‚µãƒ³ãƒ—ãƒ«, ãƒ‡ãƒ¢, ãƒ†ã‚­ã‚¹ãƒˆ\nãƒã‚¤ãƒ‹ãƒ³ã‚°, è§£æ, æ©Ÿèƒ½, streamlit, python",
                                             help="ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã«åŠ ãˆã¦ã€ã“ã“ã«å…¥åŠ›ã—ãŸå˜èªã‚‚é™¤å¤–ã•ã‚Œã¾ã™ã€‚")
final_stop_words_set = DEFAULT_STOP_WORDS_SET.copy()
if custom_stopwords_input_str.strip():
    custom_list_sw = [word.strip().lower() for word in re.split(r'[,\n]', custom_stopwords_input_str) if word.strip()] # å°æ–‡å­—åŒ–ã—ã¦çµ±ä¸€
    final_stop_words_set.update(custom_list_sw)
st.sidebar.caption(f"é©ç”¨ã•ã‚Œã‚‹ç·ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰æ•°: {len(final_stop_words_set)}")

st.sidebar.markdown("---")
st.sidebar.markdown("**å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è©³ç´°è¨­å®š**")
network_node_min_freq_val = st.sidebar.slider("ãƒãƒ¼ãƒ‰æœ€ä½å‡ºç¾æ•°:", 1, 20, 2, key="net_node_freq_slider_main")
network_edge_min_freq_val = st.sidebar.slider("ã‚¨ãƒƒã‚¸æœ€ä½å…±èµ·æ•°:", 1, 10, 2, key="net_edge_freq_slider_main")

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢: ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨å®Ÿè¡Œãƒœã‚¿ãƒ³ ---
main_text_input_area = st.text_area("ğŸ“ åˆ†æã—ãŸã„æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", height=250, 
                             value="ã“ã‚Œã¯Streamlitã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«ã§ã™ã€‚æ—¥æœ¬èªã®å½¢æ…‹ç´ è§£æã‚’è¡Œã„ã€å˜èªã®å‡ºç¾é »åº¦ãƒ¬ãƒãƒ¼ãƒˆã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãã—ã¦KWICï¼ˆæ–‡è„ˆä»˜ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰ãªã©ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚æ§˜ã€…ãªæ–‡ç« ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„ã€‚")

analyze_button_clicked = st.button("åˆ†æå®Ÿè¡Œ", type="primary", use_container_width=True)

# --- åˆ†æçµæœè¡¨ç¤ºã‚¨ãƒªã‚¢ ---
if analyze_button_clicked:
    if not main_text_input_area.strip():
        st.warning("åˆ†æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    elif tagger is None or not st.session_state.get('mecab_tagger_initialized', False):
        st.error("MeCab TaggerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã™ã‚‹ã‹ã€Streamlit Cloudã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œä¸­... ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"):
            morphemes_data_list = perform_morphological_analysis(main_text_input_area, tagger)
        
        if not morphemes_data_list:
            st.error("å½¢æ…‹ç´ è§£æã«å¤±æ•—ã—ãŸã‹ã€çµæœãŒç©ºã§ã™ã€‚å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.success(f"å½¢æ…‹ç´ è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·å½¢æ…‹ç´ æ•°: {len(morphemes_data_list)}")
            st.markdown("---")

            tab_report_view, tab_wc_view, tab_network_view, tab_kwic_view = st.tabs(["ğŸ“Š å˜èªå‡ºç¾ãƒ¬ãƒãƒ¼ãƒˆ", "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ” KWICæ¤œç´¢"])

            with tab_report_view:
                st.subheader("å˜èªå‡ºç¾ãƒ¬ãƒãƒ¼ãƒˆ")
                with st.spinner("ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­..."):
                    df_report_to_show, total_morphs, total_target_morphs = generate_word_report(morphemes_data_list, report_target_pos_selected, final_stop_words_set)
                    st.caption(f"ç·å½¢æ…‹ç´ æ•°: {total_morphs} | ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®ç•°ãªã‚Šèªæ•°: {len(df_report_to_show)} | ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®å»¶ã¹èªæ•°: {total_target_morphs}")
                    if not df_report_to_show.empty:
                        st.dataframe(df_report_to_show.style.bar(subset=['å‡ºç¾æ•°'], align='left', color='#90EE90')
                                     .format({'å‡ºç¾é »åº¦ (%)': "{:.3f}%"}))
                    else:
                        st.info("ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            with tab_wc_view:
                st.subheader("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                if FONT_PATH_FINAL:
                    with st.spinner("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆä¸­..."):
                        fig_wc_to_show = generate_wordcloud_image(morphemes_data_list, FONT_PATH_FINAL, wc_target_pos_selected, final_stop_words_set)
                        if fig_wc_to_show:
                            st.pyplot(fig_wc_to_show)
                    st.caption(f"ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆ: {os.path.basename(FONT_PATH_FINAL) if FONT_PATH_FINAL else 'æœªè¨­å®š'}")
                else:
                    st.error("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
            
            with tab_network_view:
                st.subheader("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
                if FONT_PATH_FINAL:
                    with st.spinner("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆä¸­..."):
                        html_cooc_to_show = generate_cooccurrence_network_html(
                            morphemes_data_list, main_text_input_area, tagger, FONT_PATH_FINAL,
                            net_target_pos_selected, final_stop_words_set,
                            network_node_min_freq_val, network_edge_min_freq_val)
                        if html_cooc_to_show:
                            st.components.v1.html(html_cooc_to_show, height=750, scrolling=True)
                    st.caption(f"ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆ (ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«): {os.path.basename(FONT_PATH_FINAL) if FONT_PATH_FINAL else 'æœªè¨­å®š'}")
                else:
                     st.error("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
            
            with tab_kwic_view:
                st.subheader("KWICæ¤œç´¢ (æ–‡è„ˆä»˜ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢)")
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§KWICã®å…¥åŠ›å€¤ã‚’ä¿æŒã™ã‚‹è©¦ã¿ (ã‚¿ãƒ–ã‚’åˆ‡ã‚Šæ›¿ãˆã¦ã‚‚æ¶ˆãˆãªã„ã‚ˆã†ã«)
                if 'kwic_keyword' not in st.session_state: st.session_state.kwic_keyword = ""
                if 'kwic_mode_idx' not in st.session_state: st.session_state.kwic_mode_idx = 0
                if 'kwic_window_val' not in st.session_state: st.session_state.kwic_window_val = 5

                kwic_keyword_input_val = st.text_input("KWICæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:", value=st.session_state.kwic_keyword, placeholder="æ¤œç´¢ã—ãŸã„å˜èª(åŸå½¢æ¨å¥¨)...", key="kwic_keyword_input_field")
                st.session_state.kwic_keyword = kwic_keyword_input_val # å…¥åŠ›å€¤ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜

                kwic_search_mode_options_list = ("åŸå½¢ä¸€è‡´", "è¡¨å±¤å½¢ä¸€è‡´")
                kwic_search_mode_selected_val = st.radio("KWICæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰:", kwic_search_mode_options_list, index=st.session_state.kwic_mode_idx, key="kwic_mode_radio_field")
                st.session_state.kwic_mode_idx = kwic_search_mode_options_list.index(kwic_search_mode_selected_val)


                kwic_window_val_set = st.slider("KWICè¡¨ç¤ºæ–‡è„ˆã®å½¢æ…‹ç´ æ•° (å‰å¾Œå„):", 1, 15, st.session_state.kwic_window_val, key="kwic_window_slider_field")
                st.session_state.kwic_window_val = kwic_window_val_set


                if kwic_keyword_input_val.strip():
                    search_key_type_for_kwic_val = 'åŸå½¢' if kwic_search_mode_selected_val == "åŸå½¢ä¸€è‡´" else 'è¡¨å±¤å½¢'
                    # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚å°æ–‡å­—åŒ–ã—ã¦æ¯”è¼ƒã™ã‚‹ï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚‚å°æ–‡å­—åŒ–ã—ã¦ã„ã‚‹ãŸã‚ï¼‰
                    kw_to_search = kwic_keyword_input_val.strip().lower()
                    # å½¢æ…‹ç´ ãƒªã‚¹ãƒˆã®åŸå½¢/è¡¨å±¤å½¢ã‚‚å°æ–‡å­—ã§æ¯”è¼ƒã™ã‚‹ã‹ã€å…ƒã®ã¾ã¾ã«ã™ã‚‹ã‹ã¯è¨­è¨ˆæ¬¡ç¬¬
                    # ã“ã“ã§ã¯ã€æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿å°æ–‡å­—åŒ–ã—ã€å½¢æ…‹ç´ ãƒªã‚¹ãƒˆå´ã¯å…ƒã®ã¾ã¾ï¼ˆãŸã ã—ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¯å°æ–‡å­—ã§æ¯”è¼ƒï¼‰

                    with st.spinner(f"ã€Œ{kw_to_search}ã€ã‚’æ¤œç´¢ä¸­..."):
                         # KWICæ¤œç´¢é–¢æ•°ã«æ¸¡ã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã€å…ƒã®å…¥åŠ›ã®strip()ã•ã‚ŒãŸã‚‚ã®ã‚’ä½¿ã†
                        results_kwic_list_data = perform_kwic_search(morphemes_data_list, kwic_keyword_input_val.strip(), search_key_type_for_kwic_val, kwic_window_val_set)
                    if results_kwic_list_data:
                        st.write(f"ã€Œ{kwic_keyword_input_val.strip()}ã€ã®æ¤œç´¢çµæœ ({len(results_kwic_list_data)}ä»¶):")
                        df_kwic_to_display_final = pd.DataFrame(results_kwic_list_data)
                        st.dataframe(df_kwic_to_display_final)
                    else:
                        st.info(f"ã€Œ{kwic_keyword_input_val.strip()}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆç¾åœ¨ã®æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã«ãŠã„ã¦ï¼‰ã€‚")

# --- ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ± ---
st.sidebar.markdown("---")
st.sidebar.info("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ)")
