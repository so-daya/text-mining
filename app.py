# app.py
import streamlit as st

# ãƒšãƒ¼ã‚¸è¨­å®šã¯ã€ä»–ã®ã©ã®Streamlitã‚³ãƒãƒ³ãƒ‰ã‚ˆã‚Šã‚‚å…ˆã«ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä¸€ç•ªæœ€åˆã«å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
st.set_page_config(layout="wide", page_title="ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ)")

# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import MeCab
import pandas as pd
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import networkx as nx
from pyvis.network import Network
import re
import os
import numpy as np
from itertools import combinations

# --- å®šæ•°å®šç¾© ---
MECABRC_PATH = "/etc/mecabrc"
DICTIONARY_PATH = "/var/lib/mecab/dic/ipadic-utf8"
TAGGER_OPTIONS = f"-r {MECABRC_PATH} -d {DICTIONARY_PATH}"
FONT_PATH_PRIMARY = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf' 

# â˜…â˜…â˜… ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å®šç¾© â˜…â˜…â˜…
DEFAULT_STOP_WORDS_SET = {
    # ä¸€èˆ¬çš„ãªå‹•è©ãƒ»åŠ©å‹•è©ãƒ»å½¢å¼åè©ãªã© (åŸå½¢)
    "ã™ã‚‹", "ã‚ã‚‹", "ã„ã‚‹", "ãªã‚‹", "ã„ã†", "ã§ãã‚‹", "æ€ã†", "ã‚„ã‚‹", "ãªã„", "ã‚ˆã„", "è‰¯ã„",
    "ã„ã", "æ¥ã‚‹", "ãŠã‚‹", "ã¾ã™", "ã§ã™", "ã ", "ã‚Œã‚‹", "ã‚‰ã‚Œã‚‹", "ã›ã‚‹", "ã•ã›ã‚‹", "ã„ãŸã ã",
    # ä¸€èˆ¬çš„ãªå½¢å¼åè©ãƒ»ä»£åè©ãªã©
    "ã“ã¨", "ã‚‚ã®", "ã¨ã", "ã¨ã“ã‚", "ãŸã‚", "ã‚ˆã†", "ã†ã¡", "ã»ã†", "çš„", "çš„ã ",
    "ç§", "ã‚ãªãŸ", "å½¼", "å½¼å¥³", "ã“ã‚Œ", "ãã‚Œ", "ã‚ã‚Œ", "ã“ã“", "ãã“", "ã‚ãã“", "æ–¹", "ç‚º", "è¨³", "ç­ˆ",
    # ä¸€èˆ¬çš„ã™ãã‚‹å½¢å®¹è©ãƒ»å‰¯è©ãªã©
    "å¤§ãã„", "å°ã•ã„", "é«˜ã„", "ä½ã„", "å¬‰ã—ã„", "æ¥½ã—ã„", "æ‚²ã—ã„", "åŒã˜", "æ§˜ã€…", "è‰²ã€…",
    "éå¸¸", "å¤§å¤‰", "å°‘ã—", "ã‹ãªã‚Š", "ã„ã¤ã‚‚", "ã‚ˆã", "æœ¬å½“ã«", "ã¡ã‚‡ã£ã¨", "ãŸãã•ã‚“", "å¤šã",
    # è¨˜å·é¡ (åŸå½¢ãŒãã®ã¾ã¾è¨˜å·ã«ãªã‚‹å ´åˆãŒå¤šã„)
    "/", ":", "\"", ".", ",", "ã€", "ã€‚", " ", "ã€€", # åŠè§’ãƒ»å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚‚
    "(", ")", "[", "]", "ï¼ˆ", "ï¼‰", "ã€Œ", "ã€", "ã€", "ã€‘",
    "&", "-", "_", "=", "+", "*", "%", "#", "@", "!", "?"
}

# --- MeCab Taggerã®åˆæœŸåŒ– (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨) ---
# app.py ã® initialize_mecab_tagger é–¢æ•°ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ä¿®æ­£

@st.cache_resource
def initialize_mecab_tagger():
    # --- ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º ---
    st.sidebar.subheader("MeCabåˆæœŸåŒ–ãƒ‡ãƒãƒƒã‚°:")
    mecabrc_exists = os.path.exists(MECABRC_PATH)
    dicdir_exists = os.path.exists(DICTIONARY_PATH)
    libmecab_path_check = "/usr/lib/x86_64-linux-gnu/libmecab.so.2" # ä¸€èˆ¬çš„ãªãƒ‘ã‚¹
    libmecab_exists = os.path.exists(libmecab_path_check)

    st.sidebar.text(f"mecabrc ({MECABRC_PATH}):\n  {'å­˜åœ¨ã™ã‚‹' if mecabrc_exists else 'å­˜åœ¨ã—ãªã„'}")
    st.sidebar.text(f"è¾æ›¸Dir ({DICTIONARY_PATH}):\n  {'å­˜åœ¨ã™ã‚‹' if dicdir_exists else 'å­˜åœ¨ã—ãªã„'}")
    st.sidebar.text(f"libmecab.so.2 ({libmecab_path_check}):\n  {'å­˜åœ¨ã™ã‚‹' if libmecab_exists else 'å­˜åœ¨ã—ãªã„'}")

    if dicdir_exists:
        try:
            dic_contents = os.listdir(DICTIONARY_PATH)
            st.sidebar.text(f"è¾æ›¸Dirã®å†…å®¹: {dic_contents}")
            # dicrcãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            dicrc_file_path = os.path.join(DICTIONARY_PATH, "dicrc")
            dicrc_exists = os.path.exists(dicrc_file_path)
            st.sidebar.text(f"dicrc ({dicrc_file_path}):\n  {'å­˜åœ¨ã™ã‚‹' if dicrc_exists else 'å­˜åœ¨ã—ãªã„'}")
        except Exception as e_ls:
            st.sidebar.text(f"è¾æ›¸Dirå†…å®¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e_ls}")
    # --- ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤ºã“ã“ã¾ã§ ---

    try:
        tagger_obj = MeCab.Tagger(TAGGER_OPTIONS)
        tagger_obj.parse('') 
        st.session_state['mecab_tagger_initialized'] = True
        print("MeCab Tagger initialized successfully via cache.") # ã“ã‚Œã¯ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã«å‡ºåŠ›ã•ã‚Œã¾ã™
        st.sidebar.success("MeCab TaggeråˆæœŸåŒ–æˆåŠŸ (ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚ˆã‚Š)") # UIã«ã‚‚æˆåŠŸã‚’è¡¨ç¤º
        return tagger_obj
    except Exception as e_init:
        st.error(f"MeCab Taggerã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e_init}") # UIã«ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
        st.sidebar.error(f"MeCabåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e_init}") # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚‚ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
        st.error("ãƒªãƒã‚¸ãƒˆãƒªã« `packages.txt` ãŒæ­£ã—ãè¨­å®šã•ã‚Œã€MeCabé–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ (mecab, mecab-ipadic-utf8, libmecab-dev) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.session_state['mecab_tagger_initialized'] = False
        return None

# tagger = initialize_mecab_tagger() # ã“ã®å‘¼ã³å‡ºã—ã¯å¤‰æ›´ãªã—

tagger = initialize_mecab_tagger()

# --- ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã®æ±ºå®šã¨Matplotlibã¸ã®è¨­å®š ---
FONT_PATH_FINAL = None
if 'mecab_tagger_initialized' in st.session_state and st.session_state['mecab_tagger_initialized']:
    if os.path.exists(FONT_PATH_PRIMARY):
        FONT_PATH_FINAL = FONT_PATH_PRIMARY
        st.sidebar.info(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ: {os.path.basename(FONT_PATH_FINAL)}")
        try:
            font_entry = fm.FontEntry(fname=FONT_PATH_FINAL, name=os.path.splitext(os.path.basename(FONT_PATH_FINAL))[0])
            if font_entry.name not in [f.name for f in fm.fontManager.ttflist]:
                 fm.fontManager.ttflist.append(font_entry)
            plt.rcParams['font.family'] = font_entry.name
        except Exception as e_font_setting:
            st.sidebar.error(f"Matplotlibãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e_font_setting}")
    else:
        st.sidebar.error(f"æŒ‡å®šIPAãƒ•ã‚©ãƒ³ãƒˆ '{FONT_PATH_PRIMARY}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        try:
            # Colabã‚„ä¸€éƒ¨ç’°å¢ƒã§ã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‹ã‚‰æ—¥æœ¬èªã‚’æ¢ã›ã‚‹å ´åˆãŒã‚ã‚‹
            font_names_ja = [f.name for f in fm.fontManager.ttflist if any(lang in f.name.lower() for lang in ['ipagp', 'ipag', 'takao', 'noto sans cjk jp', 'hiragino'])]
            if font_names_ja:
                FONT_PATH_FINAL = fm.findfont(fm.FontProperties(family=font_names_ja[0]))
                plt.rcParams['font.family'] = font_names_ja[0]
                st.sidebar.info(f"ä»£æ›¿æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ '{font_names_ja[0]}' ({os.path.basename(FONT_PATH_FINAL)}) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            else:
                 st.sidebar.error("åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒMatplotlibã§è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e_alt_font:
            st.sidebar.error(f"ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_alt_font}")
else:
    if 'mecab_tagger_initialized' in st.session_state and not st.session_state.get('mecab_tagger_initialized', False) :
        st.sidebar.error("MeCabãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")


# --- åˆ†æé–¢æ•°ã®å®šç¾© ---
# (perform_morphological_analysis, generate_word_report, generate_wordcloud_image, 
#  generate_cooccurrence_network_html, perform_kwic_search ã¯å‰å›ã¨å¤‰æ›´ãªã—)
# (ã“ã‚Œã‚‰ã®é–¢æ•°ã®ã‚³ãƒ¼ãƒ‰ã¯é•·ã„ãŸã‚ã€ã“ã“ã§ã¯çœç•¥ã›ãšã€å‰å›ã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆã—ã¦ãã ã•ã„)
# â†“â†“â†“â†“â†“â†“ å‰å›ã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã“ã‚Œã‚‰ã®é–¢æ•°ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã“ã“ã«è²¼ã‚Šä»˜ã‘ â†“â†“â†“â†“â†“â†“
def perform_morphological_analysis(text_input, tagger_instance):
    if tagger_instance is None or not text_input: return []
    all_morphemes = []
    node = tagger_instance.parseToNode(text_input)
    while node:
        if node.surface:
            features = node.feature.split(',')
            all_morphemes.append({
                'è¡¨å±¤å½¢': node.surface, 'åŸå½¢': features[6] if features[6] != '*' else node.surface,
                'å“è©': features[0], 'å“è©ç´°åˆ†é¡1': features[1], 'å“è©ç´°åˆ†é¡2': features[2],
                'å“è©ç´°åˆ†é¡3': features[3], 'æ´»ç”¨å‹': features[4], 'æ´»ç”¨å½¢': features[5],
                'èª­ã¿': features[7] if len(features) > 7 and features[7] != '*' else '',
                'ç™ºéŸ³': features[8] if len(features) > 8 and features[8] != '*' else ''
            })
        node = node.next
    return all_morphemes

def generate_word_report(all_morphemes, target_pos_list, stop_words_set):
    if not all_morphemes: return pd.DataFrame(), 0, 0
    report_target_morphemes = []
    for m in all_morphemes:
        if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:
            if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['éè‡ªç«‹', 'æ•°', 'ä»£åè©', 'æ¥å°¾', 'ã‚µå¤‰æ¥ç¶š', 'å‰¯è©å¯èƒ½']:
                continue
            report_target_morphemes.append(m)
    if not report_target_morphemes: return pd.DataFrame(), len(all_morphemes), 0
    word_counts = Counter(m['åŸå½¢'] for m in report_target_morphemes)
    report_data = []
    representative_info_for_report = {}
    # reversed ã‚’ä½¿ã†ã¨ãƒªã‚¹ãƒˆã®ã‚³ãƒ”ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã®ã§ã€å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯æ³¨æ„
    for m_idx in range(len(report_target_morphemes) - 1, -1, -1):
        m = report_target_morphemes[m_idx]
        if m['åŸå½¢'] not in representative_info_for_report:
            representative_info_for_report[m['åŸå½¢']] = {'å“è©': m['å“è©']} 
            
    total_all_morphemes_count_for_freq = len(all_morphemes)
    total_report_target_morphemes_count = sum(word_counts.values())
    for rank, (word, count) in enumerate(word_counts.most_common(), 1):
        info = representative_info_for_report.get(word, {}) 
        frequency = (count / total_all_morphemes_count_for_freq) * 100 if total_all_morphemes_count_for_freq > 0 else 0
        report_data.append({
            'é †ä½': rank, 'å˜èª (åŸå½¢)': word, 'å‡ºç¾æ•°': count,
            'å‡ºç¾é »åº¦ (%)': round(frequency, 3), 'å“è©': info.get('å“è©', '')
        })
    return pd.DataFrame(report_data), total_all_morphemes_count_for_freq, total_report_target_morphemes_count

def generate_wordcloud_image(all_morphemes, font_path_wc, target_pos_list, stop_words_set):
    if not all_morphemes: st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã®ãŸã‚ã®å½¢æ…‹ç´ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); return None
    if font_path_wc is None or not os.path.exists(font_path_wc): st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã«å¿…è¦ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ '{font_path_wc}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"); return None
    wordcloud_words = []
    for m in all_morphemes:
        if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:
            if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['æ•°', 'éè‡ªç«‹', 'ä»£åè©', 'æ¥å°¾']: continue
            wordcloud_words.append(m['åŸå½¢'])
    wordcloud_text_input_str = " ".join(wordcloud_words)
    if not wordcloud_text_input_str.strip(): st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰è¡¨ç¤ºå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚"); return None
    try:
        wc = WordCloud(font_path=font_path_wc, background_color="white", width=800, height=400, max_words=200, collocations=False, random_state=42).generate(wordcloud_text_input_str)
        fig, ax = plt.subplots(figsize=(12,6)); ax.imshow(wc, interpolation='bilinear'); ax.axis("off")
        return fig
    except Exception as e_wc: st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_wc}"); return None

def generate_cooccurrence_network_html(all_morphemes, text_input_co, tagger_instance, font_path_co, target_pos_list, stop_words_set, node_min_freq, edge_min_freq):
    if not all_morphemes or tagger_instance is None or not text_input_co.strip(): st.info("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚"); return None
    if font_path_co is None or not os.path.exists(font_path_co): st.error(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ©ãƒ™ãƒ«è¡¨ç¤ºã«å¿…è¦ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ '{font_path_co}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"); return None
    temp_words_for_nodes = []
    for m in all_morphemes:
        if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:
            if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['éè‡ªç«‹', 'æ•°', 'ä»£åè©', 'æ¥å°¾', 'ã‚µå¤‰æ¥ç¶š', 'å‰¯è©å¯èƒ½']: continue
            if len(m['åŸå½¢']) < 2 and m['å“è©'] != 'åè©': continue
            temp_words_for_nodes.append(m['åŸå½¢'])
    word_counts = Counter(temp_words_for_nodes); node_candidates = {word: count for word, count in word_counts.items() if count >= node_min_freq}
    if len(node_candidates) < 2: st.info(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒãƒ¼ãƒ‰ã¨ãªã‚‹å˜èªï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰ãŒ2ã¤æœªæº€ã§ã™ã€‚"); return None
    sentences = re.split(r'[ã€‚\nï¼ï¼Ÿ]+', text_input_co); sentences = [s.strip() for s in sentences if s.strip()]
    cooccurrence_counts_map = Counter()
    for sentence in sentences:
        node_s = tagger_instance.parseToNode(sentence); words_in_sentence = []
        while node_s:
            if node_s.surface:
                features = node_s.feature.split(','); original_form = features[6] if features[6] != '*' else node_s.surface
                if original_form in node_candidates: words_in_sentence.append(original_form)
            node_s = node_s.next
        for pair in combinations(sorted(list(set(words_in_sentence))), 2): cooccurrence_counts_map[pair] += 1
    if not cooccurrence_counts_map: st.info("å…±èµ·ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return None
    font_name_pyvis_graph = os.path.splitext(os.path.basename(font_path_co))[0]
    if font_name_pyvis_graph.lower() == 'ipagp': font_name_pyvis_graph = 'IPAPGothic'
    elif font_name_pyvis_graph.lower() == 'ipamp': font_name_pyvis_graph = 'IPAPMincho'
    net_graph = Network(notebook=True, height="750px", width="100%", directed=False, bgcolor="#F5F5F5", font_color="#333333")
    for word, count in node_candidates.items():
        node_s_size = int(np.sqrt(count) * 10 + 10)
        net_graph.add_node(word, label=word, size=node_s_size, title=f"{word} (å‡ºç¾æ•°: {count})", font={'face': font_name_pyvis_graph, 'size': 14, 'color': '#333333'}, borderWidth=1, color={'border': '#666666', 'background': '#D2E5FF'})
    added_edge_num = 0
    for pair_nodes, freq_cooc in cooccurrence_counts_map.items():
        if freq_cooc >= edge_min_freq:
            edge_w = float(np.log1p(freq_cooc) * 1.5 + 0.5)
            net_graph.add_edge(pair_nodes[0], pair_nodes[1], value=edge_w, title=f"å…±èµ·: {freq_cooc}å›", color={'color': '#cccccc', 'highlight': '#848484', 'opacity':0.6}); added_edge_num +=1
    if added_edge_num == 0: st.info(f"è¡¨ç¤ºå¯¾è±¡ã®å…±èµ·ãƒšã‚¢ï¼ˆå…±èµ·å›æ•° {edge_min_freq} å›ä»¥ä¸Šï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return None
    options_js_str = """ var options = {"interaction": {"navigationButtons": false, "keyboard": {"enabled": false}}, "manipulation": {"enabled": false}, "configure": {"enabled": false}, "physics": {"enabled": true, "barnesHut": {"gravitationalConstant": -30000, "centralGravity": 0.1, "springLength": 150, "springConstant": 0.03, "damping": 0.09, "avoidOverlap": 0.5}, "solver": "barnesHut", "stabilization": {"iterations": 500}}}; """
    try: net_graph.set_options(options_js_str)
    except Exception as e_set_opt: st.warning(f"Pyvisã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šã§è»½å¾®ãªã‚¨ãƒ©ãƒ¼: {e_set_opt}")
    net_graph.show_buttons(filter_=False)
    return net_graph.generate_html(name="temp_cooc_net_streamlit.html", notebook=True)

def perform_kwic_search(all_morphemes, keyword_str, search_key_type_str, window_int):
    if not keyword_str.strip() or not all_morphemes: return []
    kwic_results_data = []
    for i, morpheme_item in enumerate(all_morphemes):
        target_text_in_morpheme = morpheme_item[search_key_type_str].lower()
        keyword_to_compare = keyword_str.lower()
        if target_text_in_morpheme == keyword_to_compare:
            left_start_idx = max(0, i - window_int); left_ctx_str = "".join(m['è¡¨å±¤å½¢'] for m in all_morphemes[left_start_idx:i])
            kw_surface = morpheme_item['è¡¨å±¤å½¢']; right_end_idx = min(len(all_morphemes), i + 1 + window_int)
            right_ctx_str = "".join(m['è¡¨å±¤å½¢'] for m in all_morphemes[i+1:right_end_idx])
            kwic_results_data.append({'å·¦æ–‡è„ˆ': left_ctx_str, 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰': kw_surface, 'å³æ–‡è„ˆ': right_ctx_str})
    return kwic_results_data
# â†‘â†‘â†‘â†‘â†‘â†‘ ã“ã“ã¾ã§ã«é–¢æ•°å®šç¾©ã‚’è¨˜è¿° â†‘â†‘â†‘â†‘â†‘â†‘


# --- Streamlit UIã®ãƒ¡ã‚¤ãƒ³éƒ¨åˆ† ---
st.title("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ)")
st.markdown("æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ã€å½¢æ…‹ç´ è§£æã€å˜èªãƒ¬ãƒãƒ¼ãƒˆã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€KWICæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š ---
st.sidebar.header("âš™ï¸ åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³")
st.sidebar.markdown("**å“è©é¸æŠ (å„åˆ†æå…±é€š)**")
default_target_pos = ['åè©', 'å‹•è©', 'å½¢å®¹è©']
report_target_pos_selected = st.sidebar.multiselect("å˜èªãƒ¬ãƒãƒ¼ãƒˆ: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©', 'æ„Ÿå‹•è©', 'é€£ä½“è©'], default=default_target_pos)
wc_target_pos_selected = st.sidebar.multiselect("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©', 'æ„Ÿå‹•è©'], default=default_target_pos)
net_target_pos_selected = st.sidebar.multiselect("å…±èµ·Net: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©'], default=default_target_pos)

st.sidebar.markdown("**ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š**")
# â˜…â˜…â˜… DEFAULT_STOP_WORDS_SET ã‚’ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã®åˆæœŸå€¤ã¨ã—ã¦è¡¨ç¤º â˜…â˜…â˜…
#     ã‚»ãƒƒãƒˆã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã«å¤‰æ› (ã‚½ãƒ¼ãƒˆã—ã¦è¦‹ã‚„ã™ã)
default_stopwords_str_display = "\n".join(sorted(list(DEFAULT_STOP_WORDS_SET)))
custom_stopwords_input_str = st.sidebar.text_area("å…±é€šã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ (åŸå½¢ã‚’ã‚«ãƒ³ãƒã‚„æ”¹è¡ŒåŒºåˆ‡ã‚Šã§ç·¨é›†ã—ã¦ãã ã•ã„):", 
                                             value=default_stopwords_str_display, # åˆæœŸå€¤ã‚’è¨­å®š
                                             height=250, # è¡¨ç¤ºè¡Œæ•°ã‚’å¢—ã‚„ã™
                                             help="ã“ã“ã«å…¥åŠ›ã•ã‚ŒãŸå˜èªï¼ˆåŸå½¢ï¼‰ãŒã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦å‡¦ç†ã•ã‚Œã¾ã™ã€‚")
# â˜…â˜…â˜… ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã®å†…å®¹ã‚’ãã®ã¾ã¾æœ€çµ‚çš„ãªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆã¨ã—ã¦ä½¿ç”¨ â˜…â˜…â˜…
final_stop_words_set = set() 
if custom_stopwords_input_str.strip():
    # ã‚«ãƒ³ãƒã¾ãŸã¯æ”¹è¡Œã§åŒºåˆ‡ã‚‰ã‚ŒãŸå˜èªã‚’ãƒªã‚¹ãƒˆã«ã—ã€å„å˜èªã®å‰å¾Œã®ç©ºç™½ã‚’é™¤å»ã—ã€å°æ–‡å­—åŒ–
    custom_list_sw = [word.strip().lower() for word in re.split(r'[,\n]', custom_stopwords_input_str) if word.strip()]
    final_stop_words_set.update(custom_list_sw)
st.sidebar.caption(f"é©ç”¨ã•ã‚Œã‚‹ç·ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰æ•°: {len(final_stop_words_set)}")

st.sidebar.markdown("---")
st.sidebar.markdown("**å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è©³ç´°è¨­å®š**")
network_node_min_freq_val = st.sidebar.slider("ãƒãƒ¼ãƒ‰æœ€ä½å‡ºç¾æ•°:", 1, 20, 2, key="net_node_freq_slider_main")
network_edge_min_freq_val = st.sidebar.slider("ã‚¨ãƒƒã‚¸æœ€ä½å…±èµ·æ•°:", 1, 10, 2, key="net_edge_freq_slider_main")

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢: ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨å®Ÿè¡Œãƒœã‚¿ãƒ³ ---
main_text_input_area = st.text_area("ğŸ“ åˆ†æã—ãŸã„æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", height=250, 
                             value="ã“ã‚Œã¯Streamlitã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«ã§ã™ã€‚æ—¥æœ¬èªã®å½¢æ…‹ç´ è§£æã‚’è¡Œã„ã€å˜èªã®å‡ºç¾é »åº¦ãƒ¬ãƒãƒ¼ãƒˆã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãã—ã¦KWICï¼ˆæ–‡è„ˆä»˜ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰ãªã©ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚æ§˜ã€…ãªæ–‡ç« ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„ã€‚")

analyze_button_clicked = st.button("åˆ†æå®Ÿè¡Œ", type="primary", use_container_width=True)

# --- åˆ†æçµæœè¡¨ç¤ºã‚¨ãƒªã‚¢ ---
if analyze_button_clicked:
    if not main_text_input_area.strip():
        st.warning("åˆ†æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    elif tagger is None or not st.session_state.get('mecab_tagger_initialized', False):
        st.error("MeCab TaggerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã™ã‚‹ã‹ã€Streamlit Cloudã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œä¸­... ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"):
            morphemes_data_list = perform_morphological_analysis(main_text_input_area, tagger)
        
        if not morphemes_data_list:
            st.error("å½¢æ…‹ç´ è§£æã«å¤±æ•—ã—ãŸã‹ã€çµæœãŒç©ºã§ã™ã€‚å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.success(f"å½¢æ…‹ç´ è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·å½¢æ…‹ç´ æ•°: {len(morphemes_data_list)}")
            st.markdown("---")

            # æ„Ÿæƒ…åˆ†æã‚¿ãƒ–ã¯å‰Šé™¤æ¸ˆã¿
            tab_report_view, tab_wc_view, tab_network_view, tab_kwic_view = st.tabs([
                "ğŸ“Š å˜èªå‡ºç¾ãƒ¬ãƒãƒ¼ãƒˆ", "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ” KWICæ¤œç´¢"
            ])

            with tab_report_view:
                st.subheader("å˜èªå‡ºç¾ãƒ¬ãƒãƒ¼ãƒˆ")
                with st.spinner("ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­..."):
                    df_report_to_show, total_morphs, total_target_morphs = generate_word_report(morphemes_data_list, report_target_pos_selected, final_stop_words_set)
                    st.caption(f"ç·å½¢æ…‹ç´ æ•°: {total_morphs} | ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®ç•°ãªã‚Šèªæ•°: {len(df_report_to_show)} | ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®å»¶ã¹èªæ•°: {total_target_morphs}")
                    if not df_report_to_show.empty:
                        st.dataframe(df_report_to_show.style.bar(subset=['å‡ºç¾æ•°'], align='left', color='#90EE90')
                                     .format({'å‡ºç¾é »åº¦ (%)': "{:.3f}%"}))
                    else: 
                        st.info("ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            with tab_wc_view:
                st.subheader("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                if FONT_PATH_FINAL:
                    with st.spinner("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆä¸­..."):
                        fig_wc_to_show = generate_wordcloud_image(morphemes_data_list, FONT_PATH_FINAL, wc_target_pos_selected, final_stop_words_set)
                        if fig_wc_to_show: st.pyplot(fig_wc_to_show)
                    st.caption(f"ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆ: {os.path.basename(FONT_PATH_FINAL) if FONT_PATH_FINAL else 'æœªè¨­å®š'}")
                else: st.error("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
            
            with tab_network_view:
                st.subheader("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
                if FONT_PATH_FINAL:
                    with st.spinner("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆä¸­..."):
                        html_cooc_to_show = generate_cooccurrence_network_html(
                            morphemes_data_list, main_text_input_area, tagger, FONT_PATH_FINAL,
                            net_target_pos_selected, final_stop_words_set,
                            network_node_min_freq_val, network_edge_min_freq_val)
                        if html_cooc_to_show: st.components.v1.html(html_cooc_to_show, height=750, scrolling=True)
                    st.caption(f"ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆ (ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«): {os.path.basename(FONT_PATH_FINAL) if FONT_PATH_FINAL else 'æœªè¨­å®š'}")
                else: st.error("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
            
            with tab_kwic_view:
                st.subheader("KWICæ¤œç´¢ (æ–‡è„ˆä»˜ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢)")
                if 'kwic_keyword' not in st.session_state: st.session_state.kwic_keyword = ""
                if 'kwic_mode_idx' not in st.session_state: st.session_state.kwic_mode_idx = 0
                if 'kwic_window_val' not in st.session_state: st.session_state.kwic_window_val = 5

                kwic_keyword_input_val = st.text_input("KWICæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:", value=st.session_state.kwic_keyword, placeholder="æ¤œç´¢ã—ãŸã„å˜èª(åŸå½¢æ¨å¥¨)...", key="kwic_keyword_input_field_tab")
                st.session_state.kwic_keyword = kwic_keyword_input_val

                kwic_search_mode_options_list = ("åŸå½¢ä¸€è‡´", "è¡¨å±¤å½¢ä¸€è‡´"); kwic_search_mode_selected_val = st.radio("KWICæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰:", kwic_search_mode_options_list, index=st.session_state.kwic_mode_idx, key="kwic_mode_radio_field_tab")
                st.session_state.kwic_mode_idx = kwic_search_mode_options_list.index(kwic_search_mode_selected_val)

                kwic_window_val_set = st.slider("KWICè¡¨ç¤ºæ–‡è„ˆã®å½¢æ…‹ç´ æ•° (å‰å¾Œå„):", 1, 15, st.session_state.kwic_window_val, key="kwic_window_slider_field_tab")
                st.session_state.kwic_window_val = kwic_window_val_set

                if kwic_keyword_input_val.strip():
                    search_key_type_for_kwic_val = 'åŸå½¢' if kwic_search_mode_selected_val == "åŸå½¢ä¸€è‡´" else 'è¡¨å±¤å½¢'
                    kw_to_search = kwic_keyword_input_val.strip()
                    
                    with st.spinner(f"ã€Œ{kw_to_search}ã€ã‚’æ¤œç´¢ä¸­..."):
                        results_kwic_list_data = perform_kwic_search(morphemes_data_list, kw_to_search, search_key_type_for_kwic_val, kwic_window_val_set)
                    if results_kwic_list_data:
                        st.write(f"ã€Œ{kw_to_search}ã€ã®æ¤œç´¢çµæœ ({len(results_kwic_list_data)}ä»¶):"); df_kwic_to_display_final = pd.DataFrame(results_kwic_list_data); st.dataframe(df_kwic_to_display_final)
                    else: st.info(f"ã€Œ{kw_to_search}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆç¾åœ¨ã®æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã«ãŠã„ã¦ï¼‰ã€‚")

# --- ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ± ---
st.sidebar.markdown("---")
st.sidebar.info("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ) v0.5") # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¢ãƒƒãƒ—
