# app.py
import streamlit as st
import MeCab
import pandas as pd
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import networkx as nx
from pyvis.network import Network
import re
import os
import numpy as np
import japanize_matplotlib # Matplotlibã®æ—¥æœ¬èªåŒ–ã®ãŸã‚ (requirements.txt ã«ã‚‚è¿½åŠ )
from itertools import combinations # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ä½¿ç”¨
from IPython.core.display import HTML # Pyvisã®HTMLè¡¨ç¤ºã«ä½¿ã†å ´åˆãŒã‚ã‚‹ãŒã€Streamlitã§ã¯st.components.v1.html

# --- å®šæ•°å®šç¾© ---
# Streamlit Cloudç’°å¢ƒã§ã®MeCabã¨è¾æ›¸ã€ãƒ•ã‚©ãƒ³ãƒˆã®æ¨™æº–çš„ãªãƒ‘ã‚¹
MECABRC_PATH = "/etc/mecabrc"
DICTIONARY_PATH = "/var/lib/mecab/dic/ipadic-utf8"
# packages.txtã§fonts-ipafont-gothicã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå ´åˆã®IPA Pã‚´ã‚·ãƒƒã‚¯ã®ãƒ‘ã‚¹
FONT_PATH_PRIMARY = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf' 
TAGGER_OPTIONS = f"-r {MECABRC_PATH} -d {DICTIONARY_PATH}"

# --- MeCab Taggerã®åˆæœŸåŒ– ---
@st.cache_resource # Taggerã®åˆæœŸåŒ–ã¯ãƒªã‚½ãƒ¼ã‚¹ã‚’æ¶ˆè²»ã™ã‚‹ã®ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
def initialize_mecab_tagger():
    try:
        tagger_obj = MeCab.Tagger(TAGGER_OPTIONS)
        tagger_obj.parse('') # åˆæœŸåŒ–ã®ãŠã¾ã˜ãªã„ (BOMå¯¾ç­–ã«ã‚‚ãªã‚‹)
        st.session_state['mecab_tagger_initialized'] = True
        return tagger_obj
    except Exception as e_init:
        st.error(f"MeCab Taggerã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e_init}")
        st.error("ãƒªãƒã‚¸ãƒˆãƒªã« `packages.txt` ãŒæ­£ã—ãè¨­å®šã•ã‚Œã€MeCabé–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ (mecab, mecab-ipadic-utf8, libmecab-dev) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.session_state['mecab_tagger_initialized'] = False
        return None

tagger = initialize_mecab_tagger()

# app.py ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹æ±ºå®šéƒ¨åˆ†ã®ä¿®æ­£æ¡ˆ

# ... (MECABRC_PATH, DICTIONARY_PATH, TAGGER_OPTIONS, initialize_mecab_tagger() ã¯ãã®ã¾ã¾) ...
tagger = initialize_mecab_tagger() # å…ˆã«MeCabã‚’åˆæœŸåŒ–

# --- ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã®æœ€çµ‚æ±ºå®š ---
FONT_PATH_FINAL = None
FONT_PATH_PRIMARY = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf'

if os.path.exists(FONT_PATH_PRIMARY):
    FONT_PATH_FINAL = FONT_PATH_PRIMARY
    st.info(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ '{FONT_PATH_FINAL}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
else:
    st.warning(f"æŒ‡å®šã•ã‚ŒãŸIPAãƒ•ã‚©ãƒ³ãƒˆ '{FONT_PATH_PRIMARY}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚japanize_matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆã§ä»£æ›¿ã‚’è©¦ã¿ã¾ã™ã€‚")
    try:
        # japanize_matplotlib ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹å–å¾—ã‚’ã“ã“ã§è¡Œã†
        import japanize_matplotlib 
        alt_font_path = japanize_matplotlib.get_font_path()
        if os.path.exists(alt_font_path):
            FONT_PATH_FINAL = alt_font_path
            st.info(f"japanize_matplotlibã«ã‚ˆã‚‹ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ '{FONT_PATH_FINAL}' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        else:
            st.error("japanize_matplotlibã®ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    except ImportError as e_import_jm: # japanize_matplotlib ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆè‡ªä½“ãŒå¤±æ•—ã™ã‚‹å ´åˆ
        st.error(f"japanize_matplotlibã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e_import_jm}")
        st.error("`requirements.txt` ã« `japanize-matplotlib` ãŒæ­£ã—ãè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e_font: # get_font_path() ã§ä»–ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆ
        st.error(f"japanize_matplotlibã‹ã‚‰ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_font}")

if FONT_PATH_FINAL is None:
    st.error("æœ‰åŠ¹ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚„ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªè¡¨ç¤ºã«å•é¡ŒãŒå‡ºã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
else:
    # matplotlibã®æ—¥æœ¬èªè¨­å®š (japanize_matplotlibã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸã ã‘ã§ã¯é©ç”¨ã•ã‚Œãªã„å ´åˆãŒã‚ã‚‹ãŸã‚æ˜ç¤ºçš„ã«)
    # ãŸã ã—ã€japanize_matplotlibã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã ã‘ã§rcParamsãŒæ›´æ–°ã•ã‚Œã‚‹ã¯ãšãªã®ã§ã€
    # ã“ã“ã§ã® plt.rcParams ã®è¨­å®šã¯ä¸è¦ãªå ´åˆã‚‚å¤šã„ã€‚
    # import japanize_matplotlib ã‚’å®Ÿè¡Œã—ãŸæ™‚ç‚¹ã§æ—¥æœ¬èªåŒ–ã¯è©¦ã¿ã‚‰ã‚Œã‚‹ã€‚
    pass

# ... (ä»¥é™ã®åˆ†æé–¢æ•°ã®å®šç¾©ã‚„Streamlit UIã®å®šç¾©ã¯ç¶šã) ...
# WordCloudã‚„matplotlibã®æç”»é–¢æ•°ã«æ¸¡ã™font_pathã¯ FONT_PATH_FINAL ã‚’ä½¿ã†

# --- åˆ†æé–¢æ•°ã®å®šç¾© ---

def perform_morphological_analysis(text_input, tagger_instance):
    if tagger_instance is None or not text_input: 
        return []
    all_morphemes = []
    node = tagger_instance.parseToNode(text_input)
    while node:
        if node.surface: # BOS/EOSãƒãƒ¼ãƒ‰ã¯surfaceãŒç©ºãªã®ã§é™¤å¤–
            features = node.feature.split(',')
            all_morphemes.append({
                'è¡¨å±¤å½¢': node.surface,
                'åŸå½¢': features[6] if features[6] != '*' else node.surface,
                'å“è©': features[0],
                'å“è©ç´°åˆ†é¡1': features[1],
                'å“è©ç´°åˆ†é¡2': features[2],
                'å“è©ç´°åˆ†é¡3': features[3],
                'æ´»ç”¨å‹': features[4],
                'æ´»ç”¨å½¢': features[5],
                'èª­ã¿': features[7] if len(features) > 7 and features[7] != '*' else '',
                'ç™ºéŸ³': features[8] if len(features) > 8 and features[8] != '*' else ''
            })
        node = node.next
    return all_morphemes

def generate_word_report(all_morphemes, target_pos_list, stop_words_set):
    if not all_morphemes: 
        return pd.DataFrame()
    
    report_target_morphemes = []
    for m in all_morphemes:
        if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:
            # åè©ã®å ´åˆã®è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['éè‡ªç«‹', 'æ•°', 'ä»£åè©', 'æ¥å°¾', 'ã‚µå¤‰æ¥ç¶š', 'å‰¯è©å¯èƒ½']:
                continue
            report_target_morphemes.append(m)

    if not report_target_morphemes: 
        return pd.DataFrame()
        
    word_counts = Counter(m['åŸå½¢'] for m in report_target_morphemes)
    report_data = []
    
    # åŸå½¢ã”ã¨ã«æœ€åˆã®å‡ºç¾æ™‚ã®è©³ç´°æƒ…å ±ã‚’ä»£è¡¨ã¨ã—ã¦ä¿æŒ
    representative_info_for_report = {}
    for m in reversed(report_target_morphemes): # å¾Œã‚ã‹ã‚‰è¦‹ã¦æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã‚’æ¡ç”¨ï¼ˆç‰¹ã«æ„å‘³ã¯ãªã„ãŒä¸€è²«æ€§ã®ãŸã‚ï¼‰
        if m['åŸå½¢'] not in representative_info_for_report:
            representative_info_for_report[m['åŸå½¢']] = m
            
    total_all_morphemes_count_for_freq = len(all_morphemes) # é »åº¦è¨ˆç®—ã®æ¯æ•°ã¯å…¨å½¢æ…‹ç´ æ•°

    for rank, (word, count) in enumerate(word_counts.most_common(), 1):
        info = representative_info_for_report.get(word, {}) # åŸå½¢ã«å¯¾å¿œã™ã‚‹ä»£è¡¨æƒ…å ±ã‚’å–å¾—
        frequency = (count / total_all_morphemes_count_for_freq) * 100 if total_all_morphemes_count_for_freq > 0 else 0
        report_data.append({
            'é †ä½': rank,
            'å˜èª (åŸå½¢)': word,
            'å‡ºç¾æ•°': count,
            'å‡ºç¾é »åº¦ (%)': round(frequency, 3), # å°æ•°ç‚¹3æ¡ã«
            'å“è©': info.get('å“è©', ''),
            'å“è©ç´°åˆ†é¡1': info.get('å“è©ç´°åˆ†é¡1', ''),
            'ä»£è¡¨çš„ãªè¡¨å±¤å½¢': info.get('è¡¨å±¤å½¢', ''), # ä»£è¡¨çš„ãªè¡¨å±¤å½¢ã‚‚è¿½åŠ 
            'ä»£è¡¨çš„ãªèª­ã¿': info.get('èª­ã¿', '')   # ä»£è¡¨çš„ãªèª­ã¿ã‚‚è¿½åŠ 
        })
    return pd.DataFrame(report_data)

def generate_wordcloud_image(all_morphemes, font_path_wc, target_pos_list, stop_words_set):
    if not all_morphemes:
        st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã®ãŸã‚ã®å½¢æ…‹ç´ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return None
    if font_path_wc is None or not os.path.exists(font_path_wc):
        st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã«å¿…è¦ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ '{font_path_wc}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    
    wordcloud_words = []
    for m in all_morphemes:
        if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:
            if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['æ•°', 'éè‡ªç«‹', 'ä»£åè©', 'æ¥å°¾']:
                continue
            wordcloud_words.append(m['åŸå½¢']) # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã¯åŸå½¢ãƒ™ãƒ¼ã‚¹ãŒä¸€èˆ¬çš„
    
    wordcloud_text_input_str = " ".join(wordcloud_words)
    if not wordcloud_text_input_str.strip():
        st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰è¡¨ç¤ºå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚")
        return None

    try:
        wc = WordCloud(
            font_path=font_path_wc,
            background_color="white",
            width=800, height=400,
            max_words=200,
            collocations=False, 
            random_state=42 
        ).generate(wordcloud_text_input_str)
        
        fig, ax = plt.subplots(figsize=(12,6))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis("off")
        return fig
    except Exception as e_wc:
        st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_wc}")
        return None

def generate_cooccurrence_network_html(all_morphemes, text_input_co, tagger_instance, font_path_co, target_pos_list, stop_words_set, node_min_freq, edge_min_freq):
    if not all_morphemes or tagger_instance is None or not text_input_co.strip():
        st.info("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        return None
    if font_path_co is None or not os.path.exists(font_path_co):
        st.error(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ©ãƒ™ãƒ«è¡¨ç¤ºã«å¿…è¦ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ '{font_path_co}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    temp_words_for_nodes = []
    for m in all_morphemes:
        if m['å“è©'] in target_pos_list and m['åŸå½¢'] not in stop_words_set:
            if m['å“è©'] == 'åè©' and m['å“è©ç´°åˆ†é¡1'] in ['éè‡ªç«‹', 'æ•°', 'ä»£åè©', 'æ¥å°¾', 'ã‚µå¤‰æ¥ç¶š', 'å‰¯è©å¯èƒ½']:
                continue
            if len(m['åŸå½¢']) < 2 and m['å“è©'] != 'åè©': # åè©ä»¥å¤–ã®1æ–‡å­—ã®åŸå½¢ã¯é™¤å¤–
                continue
            temp_words_for_nodes.append(m['åŸå½¢'])

    word_counts = Counter(temp_words_for_nodes)
    node_candidates = {word: count for word, count in word_counts.items() if count >= node_min_freq}

    if len(node_candidates) < 2:
        st.info(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒãƒ¼ãƒ‰ã¨ãªã‚‹å˜èªï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰ãŒ2ã¤æœªæº€ã§ã™ã€‚")
        return None

    sentences = re.split(r'[ã€‚\nï¼ï¼Ÿ]+', text_input_co)
    sentences = [s.strip() for s in sentences if s.strip()]
    cooccurrence_counts_map = Counter()
    for sentence in sentences:
        node_s = tagger_instance.parseToNode(sentence)
        words_in_sentence = []
        while node_s:
            if node_s.surface:
                features_s = node_s.feature.split(',')
                original_form_s = features_s[6] if features_s[6] != '*' else node_s.surface
                if original_form_s in node_candidates:
                    words_in_sentence.append(original_form_s)
            node_s = node_s.next
        for pair in combinations(sorted(list(set(words_in_sentence))), 2):
            cooccurrence_counts_map[pair] += 1
    
    if not cooccurrence_counts_map:
        st.info("å…±èµ·ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    font_name_for_pyvis_graph = os.path.splitext(os.path.basename(font_path_co))[0]
    if font_name_for_pyvis_graph.lower() == 'ipagp': font_name_for_pyvis_graph = 'IPAPGothic'
    elif font_name_for_pyvis_graph.lower() == 'ipamp': font_name_for_pyvis_graph = 'IPAPMincho'
    
    net_graph = Network(notebook=True, height="750px", width="100%", directed=False, bgcolor="#F5F5F5", font_color="#333333")
    for word, count in node_candidates.items():
        node_s = int(np.sqrt(count) * 10 + 10)
        net_graph.add_node(word, label=word, size=node_s, title=f"{word} (å‡ºç¾æ•°: {count})",
                           font={'face': font_name_for_pyvis_graph, 'size': 14, 'color': '#333333'},
                           borderWidth=1, color={'border': '#666666', 'background': '#D2E5FF'})
    
    added_edge_num = 0
    for pair_nodes, freq_cooc in cooccurrence_counts_map.items():
        if freq_cooc >= edge_min_freq:
            edge_w = float(np.log1p(freq_cooc) * 1.5 + 0.5)
            net_graph.add_edge(pair_nodes[0], pair_nodes[1], value=edge_w, title=f"å…±èµ·: {freq_cooc}å›",
                               color={'color': '#cccccc', 'highlight': '#848484', 'opacity':0.6})
            added_edge_num +=1
    
    if added_edge_num == 0:
        st.info(f"è¡¨ç¤ºå¯¾è±¡ã®å…±èµ·ãƒšã‚¢ï¼ˆå…±èµ·å›æ•° {edge_min_freq} å›ä»¥ä¸Šï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    options_js = """ var options = {"interaction": {"navigationButtons": false, "keyboard": {"enabled": false}}, "manipulation": {"enabled": false}, "configure": {"enabled": false}, "physics": {"enabled": true, "barnesHut": {"gravitationalConstant": -30000, "centralGravity": 0.1, "springLength": 150, "springConstant": 0.03, "damping": 0.09, "avoidOverlap": 0.5}, "solver": "barnesHut", "stabilization": {"iterations": 500}}}; """
    try:
        net_graph.set_options(options_js)
    except Exception as e_set_opt:
        st.warning(f"Pyvisã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šã§è»½å¾®ãªã‚¨ãƒ©ãƒ¼: {e_set_opt}")
    net_graph.show_buttons(filter_=False)
    
    # HTMLæ–‡å­—åˆ—ã‚’ç›´æ¥å–å¾—
    html_content = net_graph.generate_html(name="temp_cooc_net_streamlit.html", notebook=True)
    return html_content


def perform_kwic_search(all_morphemes, keyword_str, search_key_type_str, window_int):
    if not keyword_str.strip() or not all_morphemes: 
        return []
    kwic_results_data = []
    for i, morpheme_item in enumerate(all_morphemes):
        if morpheme_item[search_key_type_str] == keyword_str:
            left_start_idx = max(0, i - window_int)
            left_ctx_str = "".join(m['è¡¨å±¤å½¢'] for m in all_morphemes[left_start_idx:i])
            kw_surface = morpheme_item['è¡¨å±¤å½¢']
            right_end_idx = min(len(all_morphemes), i + 1 + window_int)
            right_ctx_str = "".join(m['è¡¨å±¤å½¢'] for m in all_morphemes[i+1:right_end_idx])
            kwic_results_data.append({'å·¦æ–‡è„ˆ': left_ctx_str, 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰': kw_surface, 'å³æ–‡è„ˆ': right_ctx_str})
    return kwic_results_data

# --- Streamlit UIã®å®šç¾© ---
st.set_page_config(layout="wide", page_title="ç°¡æ˜“ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
st.title("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (Streamlitç‰ˆ)")
st.markdown("æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ã€å½¢æ…‹ç´ è§£æã€å˜èªãƒ¬ãƒãƒ¼ãƒˆã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€KWICæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")

# --- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ ---
DEFAULT_STOP_WORDS_SET = {
    "ã™ã‚‹", "ã‚ã‚‹", "ã„ã‚‹", "ãªã‚‹", "ã„ã†", "ã§ãã‚‹", "æ€ã†", "ã‚„ã‚‹", "ãªã„", "ã‚ˆã„", "è‰¯ã„",
    "å¤§ãã„", "å°ã•ã„", "é«˜ã„", "ä½ã„", "å¬‰ã—ã„", "æ¥½ã—ã„", "æ‚²ã—ã„", "åŒã˜", "æ§˜ã€…",
    "ã“ã¨", "ã‚‚ã®", "ãŸã‚", "ã‚ˆã†", "çš„", "çš„ã ", "ã¨ã", "ã¨ã“ã‚", "ã»ã†", "ãªã‹", "ã†ã¡",
    "ç§", "ã‚ãªãŸ", "å½¼", "å½¼å¥³", "ã“ã‚Œ", "ãã‚Œ", "ã‚ã‚Œ", "ã“ã“", "ãã“", "ã‚ãã“", "æ–¹", "ç‚º",
    "éå¸¸", "å¤§å¤‰", "å°‘ã—", "ã‹ãªã‚Š", "è‰²ã€…", "ã„ã¤ã‚‚", "ã„ãŸã ã", "/", ":", "ã‚Œã‚‹", "\""
}

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š ---
st.sidebar.header("âš™ï¸ åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³")
st.sidebar.markdown("**å…¨èˆ¬è¨­å®š**")
report_target_pos = st.sidebar.multiselect("å˜èªãƒ¬ãƒãƒ¼ãƒˆ: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©', 'æ„Ÿå‹•è©', 'é€£ä½“è©', 'æ¥ç¶šè©', 'åŠ©è©', 'åŠ©å‹•è©', 'è¨˜å·', 'ãã®ä»–'], default=['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©'])
wc_target_pos = st.sidebar.multiselect("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©', 'æ„Ÿå‹•è©'], default=['åè©', 'å‹•è©', 'å½¢å®¹è©'])
net_target_pos = st.sidebar.multiselect("å…±èµ·Net: å¯¾è±¡å“è©", ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©'], default=['åè©', 'å‹•è©', 'å½¢å®¹è©'])

custom_stopwords_str = st.sidebar.text_area("å…±é€šã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ (åŸå½¢ã‚’ã‚«ãƒ³ãƒã‚„æ”¹è¡ŒåŒºåˆ‡ã‚Šã§è¿½åŠ ):", 
                                         value="ã‚µãƒ³ãƒ—ãƒ«, ãƒ‡ãƒ¢, ãƒ†ã‚­ã‚¹ãƒˆ\nãƒã‚¤ãƒ‹ãƒ³ã‚°, è§£æ, æ©Ÿèƒ½") # æ”¹è¡Œã‚‚åŒºåˆ‡ã‚Šæ–‡å­—ã¨ã—ã¦å‡¦ç†

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®æœ€çµ‚æ±ºå®š
final_stop_words = DEFAULT_STOP_WORDS_SET.copy()
if custom_stopwords_str.strip():
    custom_list = [word.strip() for word in re.split(r'[,\n]', custom_stopwords_str) if word.strip()]
    final_stop_words.update(custom_list)
st.sidebar.caption(f"é©ç”¨ã•ã‚Œã‚‹ç·ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰æ•°: {len(final_stop_words)}")

st.sidebar.markdown("---")
st.sidebar.markdown("**å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š**")
network_node_min_freq = st.sidebar.slider("ãƒãƒ¼ãƒ‰æœ€ä½å‡ºç¾æ•°:", 1, 10, 2, key="net_node_freq_slider")
network_edge_min_freq = st.sidebar.slider("ã‚¨ãƒƒã‚¸æœ€ä½å…±èµ·æ•°:", 1, 10, 2, key="net_edge_freq_slider")


# --- ãƒ¡ã‚¤ãƒ³ç”»é¢: ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨å®Ÿè¡Œãƒœã‚¿ãƒ³ ---
main_text_input = st.text_area("ğŸ“ åˆ†æã—ãŸã„æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", height=250, 
                             value="ã“ã‚Œã¯Streamlitã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«ã§ã™ã€‚æ—¥æœ¬èªã®å½¢æ…‹ç´ è§£æã‚’è¡Œã„ã€å˜èªã®å‡ºç¾é »åº¦ãƒ¬ãƒãƒ¼ãƒˆã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãã—ã¦KWICï¼ˆæ–‡è„ˆä»˜ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰ãªã©ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚æ§˜ã€…ãªæ–‡ç« ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„ã€‚")

analyze_button = st.button("åˆ†æå®Ÿè¡Œ", type="primary")

# --- åˆ†æçµæœè¡¨ç¤ºã‚¨ãƒªã‚¢ ---
if analyze_button:
    if not main_text_input.strip():
        st.warning("åˆ†æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    elif tagger is None or not st.session_state.get('mecab_tagger_initialized', False):
        st.error("MeCab TaggerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã™ã‚‹ã‹ã€ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚")
    else:
        with st.spinner("å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œä¸­... ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"):
            morphemes_result = perform_morphological_analysis(main_text_input, tagger)
        
        if not morphemes_result:
            st.error("å½¢æ…‹ç´ è§£æã«å¤±æ•—ã—ãŸã‹ã€çµæœãŒç©ºã§ã™ã€‚å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.success(f"å½¢æ…‹ç´ è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·å½¢æ…‹ç´ æ•°: {len(morphemes_result)}")
            st.markdown("---")

            # ã‚¿ãƒ–ã§å„åˆ†æçµæœã‚’è¡¨ç¤º
            tab_report, tab_wc, tab_network, tab_kwic = st.tabs(["ğŸ“Š å˜èªå‡ºç¾ãƒ¬ãƒãƒ¼ãƒˆ", "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ” KWICæ¤œç´¢"])

            with tab_report:
                st.subheader("å˜èªå‡ºç¾ãƒ¬ãƒãƒ¼ãƒˆ")
                with st.spinner("å˜èªå‡ºç¾ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­..."):
                    df_report_result = generate_word_report(morphemes_result, report_target_pos, final_stop_words)
                    if not df_report_result.empty:
                        st.dataframe(df_report_result.style.bar(subset=['å‡ºç¾æ•°'], align='left', color='#90EE90')
                                     .format({'å‡ºç¾é »åº¦ (%)': "{:.3f}%"}))
                    else:
                        st.info("ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰ã€‚")
            
            with tab_wc:
                st.subheader("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                if FONT_PATH_FINAL: # FONT_PATH_FINAL ãŒ None ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    with st.spinner("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆä¸­..."):
                        fig_wc_result = generate_wordcloud_image(morphemes_result, FONT_PATH_FINAL, wc_target_pos, final_stop_words)
                        if fig_wc_result:
                            st.pyplot(fig_wc_result)
                        # else: # é–¢æ•°å†…ã§st.infoã‚’å‡ºã—ã¦ã„ã‚‹ã®ã§ã“ã“ã§ã¯ä¸è¦
                        #    st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.caption(f"ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆ: {os.path.basename(FONT_PATH_FINAL) if FONT_PATH_FINAL else 'æœªè¨­å®š'}")
                else:
                    st.error("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã¯ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚")
            
            with tab_network:
                st.subheader("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
                if FONT_PATH_FINAL:
                    with st.spinner("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆä¸­..."):
                        html_cooc_result = generate_cooccurrence_network_html(
                            morphemes_result, main_text_input, tagger, FONT_PATH_FINAL,
                            net_target_pos, final_stop_words,
                            network_node_min_freq, network_edge_min_freq
                        )
                        if html_cooc_result:
                            st.components.v1.html(html_cooc_result, height=750, scrolling=True)
                        # else: # é–¢æ•°å†…ã§st.infoã‚’å‡ºã—ã¦ã„ã‚‹ã®ã§ã“ã“ã§ã¯ä¸è¦
                        #    st.info("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.caption(f"ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆ (ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«): {os.path.basename(FONT_PATH_FINAL) if FONT_PATH_FINAL else 'æœªè¨­å®š'}")
                else:
                     st.error("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚")
            
            with tab_kwic:
                st.subheader("KWICæ¤œç´¢ (æ–‡è„ˆä»˜ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢)")
                kwic_keyword_input = st.text_input("KWICæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:", placeholder="æ¤œç´¢ã—ãŸã„å˜èª(åŸå½¢æ¨å¥¨)...", key="kwic_keyword_input")
                # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®é¸æŠè‚¢ã‚’ä¿®æ­£
                kwic_search_mode_options = ("åŸå½¢ä¸€è‡´", "è¡¨å±¤å½¢ä¸€è‡´") 
                kwic_search_mode_selected = st.radio("KWICæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰:", kwic_search_mode_options, index=0, key="kwic_mode_radio")
                kwic_window_val = st.slider("KWICè¡¨ç¤ºæ–‡è„ˆã®å½¢æ…‹ç´ æ•° (å‰å¾Œå„):", 1, 10, 5, key="kwic_window_slider")

                if kwic_keyword_input.strip():
                    search_key_type_for_kwic = 'åŸå½¢' if kwic_search_mode_selected == "åŸå½¢ä¸€è‡´" else 'è¡¨å±¤å½¢'
                    with st.spinner(f"ã€Œ{kwic_keyword_input}ã€ã‚’æ¤œç´¢ä¸­..."):
                        results_kwic_list = perform_kwic_search(morphemes_result, kwic_keyword_input.strip(), search_key_type_for_kwic, kwic_window_val)
                    if results_kwic_list:
                        st.write(f"ã€Œ{kwic_keyword_input}ã€ã®æ¤œç´¢çµæœ ({len(results_kwic_list)}ä»¶):")
                        df_kwic_to_display = pd.DataFrame(results_kwic_list)
                        st.dataframe(df_kwic_to_display)
                    else:
                        st.info(f"ã€Œ{kwic_keyword_input}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆç¾åœ¨ã®æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã«ãŠã„ã¦ï¼‰ã€‚")

# --- ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ± (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ---
st.sidebar.markdown("---")
st.sidebar.info("ã“ã®Webã‚¢ãƒ—ãƒªã¯Streamlitã¨å„ç¨®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
